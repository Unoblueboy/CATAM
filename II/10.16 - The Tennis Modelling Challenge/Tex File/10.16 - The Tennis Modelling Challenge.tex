% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=2cm} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information
\usepackage[toc,page]{appendix}

\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{multicol}

\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}
\lstset{ 
  language=R,                     % the language of the code
  basicstyle=\ttfamily, % the size of the fonts that are used for the code
  numbers=none,                   % removes the line-numbers
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=none,                     % no frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  keywordstyle=\color{RoyalBlue},      % keyword style
  commentstyle=\color{YellowGreen},   % comment style
  stringstyle=\color{ForestGreen}      % string literal style 
}

\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

\newcommand{\logit}{\text{logit}}
\newcommand{\logistic}{\text{logistic}}

\begin{document}
\section*{10.16}
\vspace*{6cm}

\section*{10.16 The Tennis Modelling Challenge}
\subsection*{Bradley-Terry Model}
\subsubsection*{Question 1}
We are given
\begin{equation}
	\mathbb{P}\left(\text{Player }a\text{ wins a match against Player }b\right) = \frac{\exp\left(\beta_a - \beta_b\right)}{1+\exp\left(\beta_a - \beta_b\right)}
	\label{eqn:q1_model}
\end{equation}
so
\begin{alignat}{2}
	& \mathbb{P}\left(\text{Player }b\text{ wins a match against Player }a\right) &\ =\ & \frac{\exp\left(\beta_b - \beta_a\right)}{1+\exp\left(\beta_b - \beta_a\right)}
	\label{eqn:param_swap}\\
	& &\ =\ & \frac{1}{1+\exp\left(\beta_a - \beta_b\right)}\nonumber.
\end{alignat}
Therefore this Generalised Linear Model is a Bernoulli distribution where 
\begin{equation*}
	Y_i \sim \mbox{Bernoulli}\left(\mu_i\right)\quad,\ \mu_i = \mathbb{P}\left(\text{Player }a_i\text{ wins a match against Player }b_i\right)
\end{equation*}
so we have the exponential family
\begin{alignat*}{2}
	& f\left(y_i; \mu_i \right) &\ =\ & \exp\left[\ln\left({\frac{\mu_i}{1-\mu_i}}\right)y_i+ \ln\left({1-\mu_i}\right)\right] \\
	& &\ =\ & \exp\left[y_i\ln\left(\mu_i\right)+ \left(1-y_i\right)\ln\left({1-\mu_i}\right)\right]
\end{alignat*}
which has canonical link function
\begin{equation*}
	g\left(\mu_i\right) = \ln\left(\frac{\mu_i}{1-\mu_i}\right)=\text{logit}\left(\mu_i\right).
\end{equation*}
We should quickly note that $\logit$ is an invertible, monotonic function from $\left(0, 1\right)$ to $\mathbb{R}$ with
\begin{equation*}
	\logit^{-1}\left(\theta\right) = \logistic\left(\theta\right) = \frac{e^\theta}{1+e^\theta}
\end{equation*}


thus from the definition of our model, we have that
\begin{alignat*}{2}
	& \mathbb{P}\left(\text{Player }a_i\text{ wins a match against Player }b_i\right)&\ =\ & \mu_i = \logistic\left(x_i^T\beta\right) = \frac{\exp\left(x_i^T\beta\right)}{1+\exp\left(x_i^T\beta\right)} \\
	& &\ =\ & \frac{\exp\left(\beta_{a_i} - \beta_{b_i}\right)}{1+\exp\left(\beta_{a_i} - \beta_{b_i}\right)}
\end{alignat*}

Thus we deduce 
$$x_i^T\beta = \beta_{a_i} - \beta_{b_i}$$ 
where $x_i^T$ represents the $i^\text{th}$ row of the design matrix, so the $i^\text{th}$ row of the design matrix has an entry of 1 for player $a_i$, -1 for player $b_i$ and 0 otherwise.

Exchanging the order of players in Equation \ref{eqn:q1_model} gives Equation \ref{eqn:param_swap}, which we see is of the same form. Going through the above derivation again with $a_i$ and $b_i$ swapped, we see that the exponentially family and canonical link function remain the same, but all the entries in the design matrix are swapped (i.e. all entries that were previously 1 becomes -1 and vice versa).

\subsubsection*{Question 2}

Fixing the coefficient for ``Agassi A.'' at 0 is equivalent to removing the column for ``Agassi A.'' from the design matrix. Using the data from the period 2000-2014 we get the coefficients shown in Appendix \ref{appendix:coefs_1}.


Note that the choice of what coefficient we fix does not change the fitted values $\mu_i$ as the fitted values depend on the difference between distinct coefficients, so fixing the $i^\text{th}$ coefficient at 0 would be equivalent to subtracting the $i^\text{th}$ coefficient from all coefficients.

Now we can calculate the logistic loss using
\begin{alignat*}{2}
	& \text{Logistic Loss} &\ =\ & -\frac{1}{N}\sum_{i=1}^{N}{\ln{f\left(y_i, \hat{\mu}_i\right)}} \\
	& &\ =\ & -\frac{1}{N}\sum_{i=1}^{N}{y_i\ln\left(\hat{\mu}_i\right)+ \left(1-y_i\right)\ln\left({1-\hat{\mu}_i}\right)} \\
	& &\ =\ & -\frac{1}{N}\sum_{i=1}^{N}{y_i\ln\left(\text{logit}^{-1}\left(x_i^T\hat{\beta}\right)\right)+ \left(1-y_i\right)\ln\left({1-\text{logit}^{-1}\left(x_i^T\hat{\beta}\right)}\right)}
\end{alignat*}

where we've used $\hat{\mu}_i=\logit^{-1}\left(x_i^T\hat{\beta}\right)$. Doing this for both the training and the test data we get:
\verbatiminput{"../R File/question 2 LogLoss.txt"}

This value is quite small which suggests the model that we have created is quite good, and the fact that the logistic loss for both the training data and the test data are approximately equal suggests that the model hasn't overfit to the training data.

\subsubsection*{Question 3}

To find the confidence interval for $\mathbb{P}\left(\text{Roger Federer beats Andy Murray}\right)$, we note from the model that

\begin{equation*}
	\mathbb{P}\left(\text{Roger Federer beats Andy Murray}\right) = \logit^{-1}\left(x^T\beta\right)
\end{equation*}
where $x$ is a vector where the entry for Roger Federer is 1, the entry for Andy Murray is -1, and all other entries are 0, therefore to find a 68\% confidence interval for $\mathbb{P}\left(\text{Roger Federer beats Andy Murray}\right)$, we find a 68\% confidence interval for $x^T\beta$ and plug into the above.

To find a 68\% confidence interval for $x^T\beta$, we use the fact that the Maximum Likelihood Estimator $\hat{\beta}$ has asymptotic distribution

\begin{equation*}
	\hat{\beta} \sim AN_p\left(\beta,\ i^{-1}_\beta\left(\beta, \sigma^2\right)\right)
\end{equation*}
where, $i_\beta\left(\beta,\sigma^2\right)$ is the fisher information, for the parameters of $\beta$, of the generalised linear model, so 
\begin{equation*}
	x^T\hat{\beta} \sim AN_p\left(x^T\beta,\ x^Ti^{-1}_\beta\left(\beta, \sigma^2\right)x\right)
\end{equation*}

This gives 68\% confidence interval for $\beta$ of

\begin{equation}
	x^T\beta \in \left[x^T\hat{\beta} - z\sqrt{x^Ti^{-1}_\beta\left(\beta, \sigma^2\right)x},\ x^T\hat{\beta} + z\sqrt{x^Ti^{-1}_\beta\left(\beta, \sigma^2\right)x}  \right]
	\label{eqn:conf_int}
\end{equation}

where $z$ satisfies $\mathbb{P}\left(\left|Z\right| \leq z\right) = 0.68$, $Z \sim N\left(0,\ 1\right)$. For a Bernoulli generalised linear model we may approximate $i_\beta\left(\beta,\sigma^2\right)$ using $X^TWX$ where,
\begin{equation*}
	W = 
	\begin{pmatrix}
		\hat{\mu}_1\left(1-\hat{\mu}_1\right) & 0 & \cdots & 0\\
		0& \hat{\mu}_1\left(1-\hat{\mu}_1\right) & \cdots & 0\\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots &\hat{\mu}_n\left(1-\hat{\mu}_n\right) \\
	\end{pmatrix}
\end{equation*}
and $\hat{\mu}_i = \logit^{-1}\left(x_i^T\hat{\beta}\right)$, putting this all into Equation \ref{eqn:conf_int}, we get a 68\% confidence interval for $x^T\beta$ of

\begin{equation*}
	x^T\beta \in \left[x^T\hat{\beta} - z\sqrt{x^T  X^T W X  x},\ x^T\hat{\beta} + z\sqrt{x^T  X^T W X  x}  \right]
\end{equation*}

thus a 68\% confidence interval for $\mathbb{P}\left(\text{Roger Federer beats Andy Murray}\right)$ of

\begin{multline*}
	\mathbb{P}\left(\text{Roger Federer beats Andy Murray}\right) \in \left[\logit^{-1}\left(x^T\hat{\beta} - z\sqrt{x^T  X^T W X  x}\right),\right.\\
	\left.\logit^{-1}\left(x^T\hat{\beta} + z\sqrt{x^T  X^T W X  x}\right)\right]
\end{multline*}

Performing this calculation we get confidence interval
\verbatiminput{"../R File/question 3 CI.txt"}

\subsubsection*{Question 4}

For this model we will fix all the coefficients for the player ``Agassi A.'' at 0, and will also fix all coefficients relating to performance on hard courts at 0%
\footnote{
We could instead directly consider constraint that the average of $\beta_{a, s}$ over all surfaces is 0, $s$, for any fixed player $a$. This would allow for easier interpretation of all of the coefficients ($\beta_a$ would be average performance of the player, and $\beta_{a, s}$ is the relative advantage/disadvantage a player gets when playing on surface $s$), but would require more complex computation (would require minimisation with constraints, so would have to use a method like Lagrange multipliers, which require more computational power).

We should note that although directly the constraint on the average of $\beta_{a,s}$ may cause difficulties, it can be deduced by using the constraint on coefficients relating to performance on hard courts by noting the model is invariant under transformation $\beta_a \to \beta_a + c_a$, $\beta_{a, s} \to \beta_{a, s} - c_a$, where $c_a$ is allowed to vary from player to player. Thus if we first perform calculation with constraint on coefficients relating to performance on hard courts, then use the above transformation with $c_a$ as average of $\beta_{a, s}$, we get the result with the constraint that the average of $\beta_{a, s}$ over all surfaces is 0.
}. Doing this we find the coefficients shown in Appendix \ref{appendix:coefs_2}

Now calculating the logistic loss using the training data and the test data we get
\verbatiminput{"../R File/question 4 LogLoss.txt"}

This suggests that although the model is doing well on the training data, it may be over-fitting on this data, leading to a significantly worse performance on the test data. This is most likely due the fact that we have quadrupled the size of the parameter space, but have not changed the amount of training data we use, so the model can more easily fall into the trap of over-fitting.

\subsubsection*{Question 5}

We can consider the model proposed in Question 2 to be contained in the model proposed in Question 4, thus, writing $\beta = \left(\beta_0,\ \beta_1 \right)$, where $\beta_1$ represents the parameters to do with the surface of play, we may consider a hypothesis test of the form $H_0 : \beta_1 = 0$ against $H_1 : \beta_1 \neq 0$. $H_0$ represents the model presented in Question 2, and $H_1$ represents the model presented in Question 4.

Let $B = \mathbb{R}^p$, $B_0 = \left\{\beta\in B \mid \beta_1 = 0\right\}$.We consider the quantity

\begin{equation*}
	\Lambda\left(H_0\right) = 2\log\left\{\frac{\sup_{\beta^\prime\in B}L\left(\beta^\prime\right)}{\sup_{\beta^\prime\in B_0}L\left(\beta^\prime\right)}\right\} = 2\left\{\sup_{\beta^\prime\in B}\ell\left(\beta^\prime\right) - \sup_{\beta^\prime\in B_0}\ell\left(\beta^\prime\right)\right\}
\end{equation*}
Where $L$ is the likelihood function and $\ell$ is the log-likelihood function, by Wilks' theorem, 
$$\Lambda\left(H_0\right) \xrightarrow{d} \chi^2_k$$
where $k = \dim\left(B\right) - \dim\left(B_0\right) = p - p_0$. $p$ denotes the number of parameters in the model presented in Question 4, and $p_0$ denotes the number of parameters in the model presented in Question 2. Let $\hat{\beta}_0$, $\hat{\beta}_1$ be the MLE of $\beta$ in the models represented by $H_0$, $H_1$ respectively, then
\begin{alignat*}{2}
	& \sup_{\beta^\prime\in B_0}\ell\left(\beta^\prime\right) &\ =\ & \ell\left(\hat{\beta}_0\right) \\
	& \sup_{\beta^\prime\in B}\ell\left(\beta^\prime\right) &\ =\ & \ell\left(\hat{\beta}_1\right)
\end{alignat*}
so
\begin{equation*}
	\Lambda\left(H_0\right) = 2\left(\ell\left(\hat{\beta}_1\right) - \ell\left(\hat{\beta}_0\right)\right)
\end{equation*}
This can easily be calculated, and the p-value can be found using $\chi^2_{p-p_0}$,
This gives the results
\verbatiminput{"../R File/question 5 results.txt"}

This suggests that the model created in Question 4 is significantly better than the model suggested in Question 2, and we can reject the simpler model at the 1\% level. This does not agree with our cross-validation as we saw that the simpler model performed much better on the unseen test data, than the more complex model, suggesting that the more complex model has over-fit to the training data.

\subsubsection*{Question 6}

We can use the variables \verb|W1-W5| and \verb|L1-L5| to give us more information on the precise number of games each player won in a given match, thus (hopefully) allowing our model to more accurately predict the number of matches by either 
\begin{itemize}
	\item Model for each game individually instead of for each match
		\subitem From this we would get an estimate of the probability that a player wins a game, and from this derive the probability that the player wins the match given the probability they win a game
	\item Assume $\mathbb{P}\left(\text{Player }a\text{ wins a match against Player }b\right) \approx \mathbb{P}\left(\text{Player }a\text{ wins a game against Player }b\right)$
		\subitem This would allow us to just assume that the number of games where player $a$ beats player $b$, is approximately equal to the number of matches where player $a$ beat player $b$. This gives us more data to work with, and thus (hopefully) a more accurate model.
\end{itemize}

\subsection*{Regularisation}
\subsubsection*{Question 7}

When the function \verb|glmnet| standardises a design matrix, it transforms the matrix in a way such that for each $j = 1, \ldots, p$
$$ \sum_{i=1}^{N}{X_{ij}} = 0$$
and
$$ \sqrt{\sum_{i=1}^{N}{\frac{X_{ij}^2}{N}}} = 1$$
i.e. each column of the design matrix has mean 0 and standard deviation 1. This makes sense if the data is not all of the same scale, as data at larger scales are likely to dominate the fit of the model. In our model, the design matrix already has all of the columns at the correct scale, so we do not need to standardise our model.

When we use the model we get the result shown in Appendix \ref{appendix:coefs_3}, and we find the optimal value of $\lambda$ is
\verbatiminput{"../R File/question 7 minLam.txt"}

\subsubsection*{Question 8}

With the optimal value of $\lambda$, we find that the number of estimates for $\beta_{a, s}$ that are non-zero is
\verbatiminput{"../R File/question 8 nonzero.txt"}
and calculating the logistic loss for both the training and the test data we get
\verbatiminput{"../R File/question 8 LogLoss.txt"}
The logistic loss on the training for this model is slightly larger than the logistic loss for the model proposed in Question 4, however the logistic loss on the test data is significantly smaller for this model compared to the model proposed in Question 4. These results suggest that the new model generalises better than the model proposed in Question 4, i.e. adding regularisation has allowed us to generate a model which generalises much better.

\subsubsection*{Question 9}
We are going to change the model suggested in Question 2 to assume that the performance of each player changes linearly over time\footnote{
We could instead consider a model where we add parameters for each year, for each player, but if we do this we face two problems
\begin{enumerate}
	\item There's a high probability the model will over-fit to the training data provided
	\item We would need training data from all the years played
		\subitem This would prevent the model from making any predictions about future matches(and also means if we wanted to compare the model to those already suggested, we would have to retrain all of those models on the new training data)
\end{enumerate}
}, so we can introduce a parameter $\beta_{a, p}$ to be the change in performance per year of player $a$. Our new model thus becomes
\begin{equation*}
	\mathbb{P}\left(\text{Player }a\text{ wins a match against Player }b\text{ in year }t\right) = \frac{\exp\left(\beta_a + t\beta_{a, p} - \beta_b - t\beta_{b, p}\right)}{1+\exp\left(\beta_a + t\beta_{a, p} - \beta_b - t\beta_{b, p}\right)}
\end{equation*}
thus the rows of our design matrix are similar to the design matrix for the model of Question 2, with the addition that the entry representing $\beta_{a, p}$ is $t$ and the entry representing $\beta_{b, p}$ is $-t$ (all other entries are 0 as before). In order to make sure the model isn't dominated by the terms $\beta_{a, p}$ in training, we will map the years 2000-2014 to $t \in \left[0, 1\right]$, we will also use the constraint that all coefficients relating to player ``Agassi A.'' are 0. We will consider two models of this form, 
\begin{enumerate}
	\item A model with no weights
	\item A model with weights exclusively on the $\beta_{a, p}$ coefficients
\end{enumerate}

Doing this we get the coefficients shown in Appendix \ref{appendix:coefs_4} and Appendix \ref{appendix:coefs_5} respectively and the Logistic Losses
\verbatiminput{"../R File/question 9 LogLoss noWeights.txt"}
\verbatiminput{"../R File/question 9 LogLoss Weights.txt"}
Suggesting similar performance, although the model with the lasso penalty performances slightly worse in both measures.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{"../R File/federer_nadal_probs"}
	\caption{Graph showing the probability that Roger Federer beats Rafael Nadal against time}
	\label{fig:federer_vs_nadal}
\end{figure}

We can see in Figure \ref{fig:federer_vs_nadal} that as time continues onwards, the probability that Federer beats Nadal decreases.

To compare all of the models, we will look at the Logistic Loss and we will use the Akaike Information Criterion (AIC) to approximate the Kullback-Leibler Divergence (KLD) of all the models using the fact that the KLD is approximately equal to AIC$/2n$ where $n$ is the number of samples used and $n$ large. We will label the models as follows
\begin{itemize}
	\item \textbf{Model 1}: The model presented in question 2
	\item \textbf{Model 2}: The model presented in question 4
	\item \textbf{Model 3}: The model presented in question 7
	\item \textbf{Model 4}: The model presented in question 9 without the lasso penalty
	\item \textbf{Model 5}: The model presented in question 9 with the lasso penalty
\end{itemize}
As we require $n$ to be large in our approximation of the KLD, we will only approximate the KLD using the training data. Doing this we get the following results
\verbatiminput{"../R File/question 9 results.txt"}
This suggests that out of all the models we have proposed, model 4 is the best with respect to all the measures considered above.

\subsection*{Can you outperform the betting market?}
\subsubsection*{Question 10}

Let $B_{i,W}$ be the betting odds for the winning player of the $i^{th}$ match and let $B_{i, L}$ be defined similarly for the losing player. Then we can say
\begin{alignat*}{2}
	& R_{i, W} &\ \sim\ & \phi_{i, W}\left(B_{i, W}-1\right)Y_i\\
	& R_{i, L} &\ \sim\ & \phi_{i, L}\left(B_{i, L}-1\right)\left(1-Y_i\right)
\end{alignat*}
where $Y_i$ are independent Bernoulli variables (implying pairs $\left(R_{i,L}, R_{i,W}\right)$ are independent for different values of $i$) with 
$$\mathbb{P}\left(Y_i=1\right)=\mathbb{P}\left(\text{Player }W_i\text{ wins a match against Player }L_i\right)\eqqcolon\mu_{i, W}$$
$W_i$ is winner of $i^{th}$ match, $L_i$ is loser of the $i^{th}$ match, so we can write $Y_i \sim \text{Ber}\left(\mu_{i, W}\right)$.

Thus
\begin{alignat*}{2}
	& \mathbb{E}\left[R\right] &\ =\ & \mathbb{E}\left[\sum_{i=1}^{k}{\left(R_{i, W} + R_{i, L}\right)}\right]\\
	& &\ =\ & \sum_{i=1}^{k}{\left(\mathbb{E}\left[R_{i, W}\right] + \mathbb{E}\left[R_{i, L}\right]\right)}\\
	& &\ =\ & \sum_{i=1}^{k}{\phi_{i, W}\left(B_{i, W}-1\right)\mathbb{E}\left[Y_i\right] + \phi_{i, L}\left(B_{i, L}-1\right)\left(1-\mathbb{E}\left[Y_i\right]\right)}\\
	& &\ =\ & \sum_{i=1}^{k}{\phi_{i, W}\left(B_{i, W}-1\right)\mu_{i, W} + \phi_{i, L}\left(B_{i, L}-1\right)\left(1-\mu_{i, W}\right)}\\
	& &\ =\ & \omega^T q
\end{alignat*}
where $\omega = \phi$ and $q$ is a vector such that 
\begin{alignat*}{2}
& q_{i, W} &\ =\ & \left(B_{i, W}-1\right)\mu_{i, W} \\
& q_{i, L} &\ =\ & \left(B_{i, L}-1\right)\left(1-\mu_{i, W}\right)
\end{alignat*}
We also have

\begin{alignat*}{2}
	& \nu\text{Var}\left(R\right) &\ =\ & \nu\text{Var}\left(\sum_{i=1}^{k}{\left(R_{i, W} + R_{i, L}\right)}\right) \\
	& &\ =\ & \nu\sum_{i=1}^{k}\text{Var}\left(R_{i, W} + R_{i, L}\right)\\
	& &\ =\ & \nu\sum_{i=1}^{k}\text{Var}\left(\phi_{i, W}\left(B_{i, W}-1\right)Y_{i} + \phi_{i, L}\left(B_{i, L}-1\right)\left(1-Y_{i}\right)\right) \\
	& &\ =\ & \nu\sum_{i=1}^{k}{
	\begin{pmatrix}
	\phi_{i, W}\left(B_{i, W}-1\right),& \phi_{i, L}\left(B_{i, L}-1\right)
	\end{pmatrix}
	\text{Cov}
	\begin{pmatrix}
		Y_{i} \\
		1-Y_{i}
	\end{pmatrix}
	\begin{pmatrix}
	\phi_{i, W}\left(B_{i, W}-1\right)\\
	\phi_{i, L}\left(B_{i, L}-1\right)
	\end{pmatrix} }\\
	& &\ =\ & \sum_{i=1}^{k}{
	\begin{pmatrix}
	\phi_{i, W},& \phi_{i, L}
	\end{pmatrix}
	\nu A_i
	\begin{pmatrix}
	\phi_{i, W}\\
	\phi_{i, L}
	\end{pmatrix}} \\
	& &\ =\ & \omega^T Q \omega
\end{alignat*}
where $\omega = \phi$ and $Q$ is a matrix such that
\begin{equation*}
	Q = \nu
	\begin{pmatrix}
		A_1 & 0 & \cdots & 0\\
		0 & A_2 & \cdots & 0\\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & A_k \\
	\end{pmatrix}
\end{equation*}
$A_i$ is a matrix
\begin{alignat*}{2}
	& A_i &\ =\ &
	\mu_{i, W}\left(1-\mu_{i, W}\right)	
	\begin{pmatrix}
	B_{i, W}-1 & 0\\
	0 & B_{i, L}-1
	\end{pmatrix}
	\begin{pmatrix}
		1& -1 \\
		-1& 1 \\
	\end{pmatrix}
	\begin{pmatrix}
	B_{i, W}-1 & 0\\
	0 & B_{i, L}-1
	\end{pmatrix} \\
	& &\ =\ & \mu_{i, W}\left(1-\mu_{i, W}\right)
	\begin{pmatrix}
		\left(B_{i, W}-1\right)^2 & -\left(B_{i, W}-1\right)\left(B_{i, L}-1\right)\\
		-\left(B_{i, W}-1\right)\left(B_{i, L}-1\right) & \left(B_{i, L}-1\right)^2
	\end{pmatrix}
\end{alignat*}

So given $B_{i,W}, B_{i,L} \geq 1 \ \;\forall i=1,\ldots,k$, then $A_i$ positive semi-definite for all $i=1,\ldots,k$, thus $Q$ positive semi-definite. Note that the conditions 
\begin{gather*}
	\phi_{i, W}, \phi_{i, L} \geq 0 \text{ for }i=1,\ldots,k\\
	\sum_{i=1}^{k}{\phi_{i, W} + \phi_{i, L}} = 1000
\end{gather*}
imply
\begin{gather*}
	\omega_{i} \geq 0 \text{ for }i=1,\ldots,2k\\
	\omega^T\mathbf{1} = 1000
\end{gather*}
thus the problem
\begin{alignat*}{2}
	&\underset{\phi \in \mathbb{R}^{2k}}{\text{minimise}} &\ & -\mathbb{E}\left[R\right] + \nu\text{Var}\left(R\right) \\
	&\text{subject to} &\ & \phi_{i, W}\geq 0, \phi_{i, L} \geq 0 \text{ for }i=1,\ldots,k, \ \sum_{i=1}^{k}{\left[\phi_{i, W} + \phi_{i, L}\right]} = 1000
\end{alignat*}
can be written as
\begin{alignat*}{2}
	&\underset{\omega \in \mathbb{R}^{2k}}{\text{minimise}} &\ & \omega^T Q \omega - \omega^Tq \\
	&\text{subject to} &\ & \omega^T \mathbf{1} = 1000,\ \omega_{i}\geq 0\text{ for }i=1,\ldots,2k
\end{alignat*}
where $q, Q$ are written in terms of $B_{i, W}$, $B_{i, L}$ which are given to us, and $\mu_{i, W}$ which can be predicted from a logistic regression model.

\subsubsection*{Question 11}

The code to find the optimal portfolio can be found in the program Listings section.

For the total profit, I considered both Models 2 and 3, the results of which can be found in Figure \ref{fig:markowitz_portfolio}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{"../R File/markowitz_portfolio"}
	\caption{Graph showing the total profit made from the optimal Markowitz Portfolio for Models 2 and 3}
	\label{fig:markowitz_portfolio}
\end{figure}

We can see that for all values of $\nu$ that the predictions from Model 3 seem to outperform the predictions from Model 2, which is to be expected as we found that Model 3 seemed to generalise better than Model 2. For model 2, we see that from $\nu = 10^{-3}$ to $\nu = 10^2$ the total profit increases, this suggests that for small values of $\nu$ the money is being mainly placed on risky bets and is then lost. We also see that as $\nu$ increases past $\nu=10^2$ the total profit quickly decreases. So for Model 2, we see that the optimal value of $\nu$ to use is $\nu = 10^2$

\subsubsection*{Question 12}
Looking through our argument in Question 10, we can see that we assumed to know the exact value of $\mathbb{E}\left[Y_i\right]$ in order to have an easily written down form for the vector $q$ and the matrix $Q$, so we can simply write down the more general form of $q$, and $Q$

The $q \in \mathbb{R}^{2k}$ is such that
\begin{alignat*}{2}
& q_{i, W} &\ =\ & \left(B_{i, W}-1\right)\mathbb{E}\left[Y_i\right] \\
& q_{i, L} &\ =\ & \left(B_{i, L}-1\right)\left(1-\mathbb{E}\left[Y_i\right]\right)
\end{alignat*}
where $\mathbb{E}\left[Y_i\right]$ calculated using posterior distribution of $\mu_{i,W}$,

The matrix $Q$ is such that
\begin{equation*}
	Q = \nu
	\begin{pmatrix}
		A_1 & 0 & \cdots & 0\\
		0 & A_2 & \cdots & 0\\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & A_k \\
	\end{pmatrix}
\end{equation*}
where $A_i$ are matrices such that
\begin{alignat*}{2}
	& A_i &\ =\ &
	\begin{pmatrix}
	B_{i, W}-1 & 0\\
	0 & B_{i, L}-1
	\end{pmatrix}
	\text{Cov}
	\begin{pmatrix}
		Y_{i} \\
		1-Y_{i}
	\end{pmatrix}
	\begin{pmatrix}
	B_{i, W}-1 & 0\\
	0 & B_{i, L}-1
	\end{pmatrix} \\
\end{alignat*}
where $\text{Cov}\left(Y_{i}, 1-Y_{i}\right)$ again to be calculated using the posterior distribution of $\mu_{i,W}$. This leaves us with a quadratic optimisation problem with affine constraints.

\subsubsection*{Question 13}

Assume that before the $i^{th}$ bet, we have a total bankroll of 
$$b_i,$$ 
then after this bet, we would have a total bankroll of 
\begin{alignat*}{2}
	& R_i &\ \coloneqq\ & b_i\left(1-\left(\phi_{i, W} + \phi_{i, L}\right)\right) + b_i\phi_{i, W}B_{i, W}Y_i + b_i\phi_{i, L}B_{i, L}\left(1-Y_i\right) \\
	& &\ =\ & b_i\left(\left(1-\left(\phi_{i, W} + \phi_{i, L}\right)\right) + \phi_{i, W}B_{i, W}Y_i + \phi_{i, L}B_{i, L}\left(1-Y_i\right)\right)
\end{alignat*}
where $Y_i$ is defined the same as in Question 10. For the Kelly Criterion we want to maximise
\begin{alignat*}{2}
	&\mathbb{E}\left[\log{R_i}\right] &\ =\ & \mathbb{E}\left[\log\left(b_i\left(\left(1-\left(\phi_{i, W} + \phi_{i, L}\right)\right) + \phi_{i, W}B_{i, W}Y_i + \phi_{i, L}B_{i, L}\left(1-Y_i\right)\right)\right)\right] \\
	& &\ =\ & \log{b_i} + \mu_{i, W}\log\left(1-\phi_{i, L} + \phi_{i, W}\left(B_{i, W}-1\right)\right)\\
	&&& \qquad \ + \left(1-\mu_{i, W}\right)\log\left(1-\phi_{i, W} + \phi_{i, L}\left(B_{i, L}-1\right)\right)
\end{alignat*}
which is equivalent to maximising
\begin{equation*}
	\mu_{i, W}\log\left(1-\phi_{i, L} + \phi_{i, W}\left(B_{i, W}-1\right)\right) + \left(1-\mu_{i, W}\right)\log\left(1-\phi_{i, W} + \phi_{i, L}\left(B_{i, L}-1\right)\right).
\end{equation*}
We can approximate $\mu_{i, W}$ using the logistic models defined above, and thus find approximate solutions to the above maximisation problem. Doing this using Model 2 to find approximations from $\mu_{i,W}$, we get the graph shown in Figure \ref{fig:kelly_criterion}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{"../R File/question 13 bankroll"}
	\caption{Graph showing the Bankroll over time using the Kelly Fractions generated using approximations from Model 2}
	\label{fig:kelly_criterion}
\end{figure}

\clearpage
\begin{appendices}
\section{Large Program Outputs}
\subsection{No Regularisation}
\subsubsection{Coefficients for the Generalised Linear Model Presented in Question 2}
\label{appendix:coefs_1}
\begin{multicols}{2}
\verbatiminput{"../R File/question 2 coefs.txt"}
\end{multicols}

\subsubsection{Coefficients for the Generalised Linear Model Presented in Question 4}
\label{appendix:coefs_2}
\begin{multicols}{2}
\verbatiminput{"../R File/question 4 coefs.txt"}
\end{multicols}

\subsection{Regularisation}
\subsubsection{Coefficients for the Generalised Linear Model Presented in Question 7}
\label{appendix:coefs_3}
\begin{multicols}{2}
\verbatiminput{"../R File/question 7 coefs.txt"}
\end{multicols}

\subsubsection{Coefficients for the Generalised Linear Model Presented in Question 9 Without Lasso Penalty}
\label{appendix:coefs_4}
\begin{multicols}{2}
\verbatiminput{"../R File/question 9 coefs 1.txt"}
\end{multicols}

\subsubsection{Coefficients for the Generalised Linear Model Presented in Question 9 With Lasso Penalty}
\label{appendix:coefs_5}
\begin{multicols}{2}
\verbatiminput{"../R File/question 9 coefs 2.txt"}
\end{multicols}

\end{appendices}
\clearpage
\section*{Programs}
\subsection*{Bradley-Terry Model}
\subsubsection*{Code to Split all the Data Received into Well Formatted Training and Test Data}
\begin{lstlisting}
Tennis <- read.csv("http://www.damtp.cam.ac.uk/user/catam/data/II-10-16-2019-mensResults.csv")
Tennis$Date <- as.Date(Tennis$Date,format='%d/%m/%y')
lam_seq = c(exp(seq(log(1), log(1e-10), length.out = 1000)), 0)

invlogit <- function(x) exp(x) / (1 + exp(x))
K <- function(theta) log(1 + exp(theta))

Training_Data <- Tennis[(Tennis$Date <= as.Date("2014/12/31")) & (!is.na(Tennis$Date)), ]
Training_length = dim(Training_Data)[1]
Test_Data <- Tennis[(Tennis$Date >= as.Date("2015/01/01")) & (!is.na(Tennis$Date)), ]
Test_length = dim(Test_Data)[1]

tennis_names = levels(Tennis$Winner)
Training_winners <- as.vector(Training_Data$Winner)
Training_losers <- as.vector(Training_Data$Loser)
Test_winners <- as.vector(Test_Data$Winner)
Test_losers <- as.vector(Test_Data$Loser)
Training_Y <- rep(c(1,0), length.out = Training_length)
Test_Y <- rep(c(1,0), length.out = Test_length)
\end{lstlisting}
\subsubsection*{Code to Generate and Evaluate Model 1}
\begin{lstlisting}
X <- matrix(data=0, nrow=Training_length, ncol = length(tennis_names)-1)
colnames(X) <- tennis_names[-1]

for (i in 1:Training_length) {
  winner = Training_winners[i]
  loser = Training_losers[i]
  y_factor = Training_Y[i]*2-1
  if (winner != tennis_names[1]) {
    X[i, winner] = 1 * y_factor
  }
  if (loser != tennis_names[1]) {
    X[i, loser] = -1 * y_factor
  }
}

# Make X a sparse Matrix
X <- as(X, "sparseMatrix")

TennisGLM1 <- glmnet(X, Training_Y, lambda=0, intercept=FALSE, family="binomial", standardize = FALSE, thresh = 1e-14)
summary(TennisGLM1)

coefs <- as.vector(coef(TennisGLM1, s=0))
coefs <- coefs[2: length(coefs)]
names(coefs) <- colnames(X)
write.table(coefs, "question 2 coefs.txt", col.names = F, sep = " : ", quote = F)

LL1 = 0
for (i in 1:Training_length) {
  row_X = X[i, ]
  Xb = row_X %*% coefs
  term = Training_Y[i]*log(invlogit(Xb)) - (1-Training_Y[i])*K(Xb)
  LL1 = LL1 + term
}
LL1 = LL1*-1/Training_length
print(paste("TennisGLM1 Log Loss for Training Set:", LL1))


Test_X <- matrix(data=0, nrow=Test_length, ncol = length(tennis_names)-1)
colnames(Test_X) <- tennis_names[-1]

for (i in 1:Test_length) {
  winner = Test_winners[i]
  loser = Test_losers[i]
  y_factor = Test_Y[i]*2-1
  if (winner != tennis_names[1]) {
    Test_X[i, winner] = 1 * y_factor
  }
  if (loser != tennis_names[1]) {
    Test_X[i, loser] = -1 * y_factor
  }
}

LL2 = 0
for (i in 1:Test_length) {
  row_Test_X = Test_X[i, ]
  Test_Xb = row_Test_X %*% coefs
  term = Test_Y[i]*log(invlogit(Test_Xb)) - (1-Test_Y[i])*K(Test_Xb)
  LL2 = LL2 + term
}
LL2 = LL2*-1/Test_length
print(paste("TennisGLM1 Log Loss for Test Set:", LL2))

write(sprintf("Training Data Logistic Loss %.15f
Test Data Logistic Loss %.15f", LL1, LL2), file = "question 2 LogLoss.txt")
\end{lstlisting}
\subsubsection*{Code to Find 68\% Confidence interval for the probability that Federer beats Murray}
\begin{lstlisting}
d <- rep(0, 59)
names(d) <- tennis_names[-1]
d["Federer R."] = 1
d["Murray A."] = -1
pred <- t(d) %*% coefs
critval <- qnorm(0.5 + 0.68/2)

# W = matrix(0, nrow = Training_length, ncol = Training_length)
W_diag = rep(0, Training_length)
for (i in 1:Training_length) {
  theta = t(X[i, ]) %*% coefs
  W_diag[i] = invlogit(theta)*(1-invlogit(theta))
}
W = diag(W_diag)
Covar_mat = solve(t(X) %*% W %*% X)
se = sqrt(as.numeric(t(d) %*% Covar_mat %*% d))
CI_prob_Bounds <- invlogit(c(pred - critval*se, pred + critval*se))
CI_string = paste0("Confidence interval the for probability that Roger Federer beats Andy Murray is 
[",
                   CI_prob_Bounds[1],", ", CI_prob_Bounds[2], "]")
print(CI_string)

write(CI_string, file = "question 3 CI.txt")
\end{lstlisting}
\subsubsection*{Code to Generate and Evaluate Model 2}
\begin{lstlisting}
tennis_surfaces = levels(Tennis$Surface)
column_names = rep("",(length(tennis_names)-1)*(length(tennis_surfaces)))
X2 <- matrix(data=0, nrow=Training_length, ncol = length(column_names))
for (i in 2:length(tennis_names)) {
  column_names[4*(i-2)+1] <- tennis_names[i]
  for (j in 2:length(tennis_surfaces))
    column_names[4*(i-2)+(j)] <- paste(tennis_names[i], tennis_surfaces[j-1])
}
colnames(X2) <- column_names

Training_surfaces <- as.vector(Training_Data$Surface)
Test_surfaces <- as.vector(Test_Data$Surface)

for (i in 1:Training_length) {
  winner = Training_winners[i]
  loser = Training_losers[i]
  surface = Training_surfaces[i]
  y_factor = 2*Training_Y[i]-1
  if (winner != tennis_names[1]) {
    X2[i, winner] <- 1*y_factor
    if (surface != tennis_surfaces[4]) {
      X2[i, paste(winner, surface)] <- 1*y_factor
    }
  }
  if (loser != tennis_names[1]) {
    X2[i, loser] <- -1*y_factor
    if (surface != tennis_surfaces[4]) {
      X2[i, paste(loser, surface)] <- -1*y_factor
    }
  }
}
# Delete empty columns
X2 <- X2[, colSums(X2==0) != nrow(X2)]
# Make X2 a sparse Matrix
X2 <- as(X2, "sparseMatrix")

TennisGLM2 <- glmnet(X2, Training_Y, lambda=lam_seq, intercept=FALSE, 
                     family="binomial", standardize = FALSE, thresh = 1e-12, maxit = 1e5)
summary(TennisGLM2)


coefs2 <- as.vector(coef(TennisGLM2, s=0))
coefs2 <- coefs2[2: length(coefs2)]
names(coefs2) <- colnames(X2)

write.table(coefs2, "question 4 coefs.txt", col.names = F, sep = ": ", quote = F)

LL3 = 0
for (i in 1:Training_length) {
  row_X2 = X2[i, ]
  X2b = row_X2 %*% coefs2
  term = Training_Y[i]*log(invlogit(X2b)) - (1-Training_Y[i])*K(X2b)
  LL3 = LL3 + term
}
LL3 = LL3*-1/Training_length
print(paste("TennisGLM2 Log Loss for Training Set:", LL3))

Test_X2 <- matrix(data=0, nrow=Test_length, ncol = (length(tennis_names)-1)*(length(tennis_surfaces)))
colnames(Test_X2) <- column_names

for (i in 1:Test_length) {
  winner = Test_winners[i]
  loser = Test_losers[i]
  surface = Test_surfaces[i]
  y_factor = 2*Test_Y[i]-1
  if (winner != tennis_names[1]) {
    Test_X2[i, winner] <- 1*y_factor
    if (surface != tennis_surfaces[4]) {
      Test_X2[i, paste(winner, surface)] <- 1*y_factor
    }
  }
  if (loser != tennis_names[1]) {
    Test_X2[i, loser] <- -1*y_factor
    if (surface != tennis_surfaces[4]) {
      Test_X2[i, paste(loser, surface)] <- -1*y_factor
    }
  }
}
# Delete columns not in X2
Test_X2 <- Test_X2[, colnames(Test_X2) %in% colnames(X2)]

LL4 = 0
for (i in 1:Test_length) {
  row_Test_X2 = Test_X2[i, ]
  Test_X2b = row_Test_X2 %*% coefs2
  term = Test_Y[i]*log(invlogit(Test_X2b)) - (1-Test_Y[i])*K(Test_X2b)
  LL4 = LL4 + term
}
LL4 = LL4*-1/Test_length
print(paste("TennisGLM2 Log Loss for Test Set:", LL4))

write(sprintf("Training Data Logistic Loss %.15f
Test Data Logistic Loss %.15f", LL3, LL4), file = "question 4 LogLoss.txt")
\end{lstlisting}
\subsubsection*{Code to Perform a Formal Hypothesis Test on Models 1 and 2}
\begin{lstlisting}
test_stat = (LL1*Training_length*2-LL3*Training_length*2)
# Variance known to be 1 so use chi squared dist
p_val = 1-pchisq(test_stat, dim(X2)[2]-dim(X)[2])# length(coefs2)-length(coefs))
print(paste("P-value for the likelihood ratio test is:",p_val))
write(sprintf("Test Statistic: %.15f
P-value for the likelihood ratio test: %.15f", test_stat, p_val), file = "question 5 results.txt")
\end{lstlisting}
\subsection*{Regularisation}
\subsubsection*{Code to Generate Model 3}
\begin{lstlisting}
w <- rep(c(0,1,1,1), length(tennis_names)-1)
names(w) <- column_names
w <- w[names(w) %in% colnames(X2)]

TennisLassoGLM1 <- cv.glmnet(X2, Training_Y, family="binomial", alpha=1, penalty.factor=w, intercept=FALSE,
                             standardize = FALSE, thresh = 1e-14, maxit = 1e5, nfolds = 10)
min_lambda = TennisLassoGLM1$lambda.min
temp_coefs = coef(TennisLassoGLM1, s=min_lambda)
coefs3 <- as.vector(temp_coefs)
names(coefs3) <- rownames(temp_coefs)
coefs3 <- coefs3[-1]
print(paste("Minimum Lambda is:", min_lambda))

write.table(coefs3, "question 7 coefs.txt", col.names = F, sep = ": ", quote = F)
write(paste("Lambda which minimises the mean cross-validated error is:", min_lambda), file = "question 7 minLam.txt")
\end{lstlisting}
\subsubsection*{Code to Find the Number of Non-Zero Surface Terms in the Coefficients Found for Model 3}
\begin{lstlisting}
non_zero_surface_terms = 0
for (surface_param in names(coefs3)[!names(coefs3) %in% tennis_names]) {
  if (coefs3[surface_param] != 0) {
    non_zero_surface_terms = non_zero_surface_terms + 1
  }
}
print(paste("The number of non zero surface terms are:", non_zero_surface_terms))
write(paste("The number of non zero surface terms are:", non_zero_surface_terms), file="question 8 nonzero.txt")
\end{lstlisting}
\subsubsection*{Code to Evaluate Model 3}
\begin{lstlisting}
LL5 = 0
for (i in 1:Training_length) {
  row_X2 = X2[i, ]
  X2b = row_X2 %*% coefs3
  term = Training_Y[i]*log(invlogit(X2b)) - (1-Training_Y[i])*K(X2b)
  LL5 = LL5 + term
}
LL5 = LL5*-1/Training_length
print(paste("TennisLassoGLM1 Log Loss for Training Set:", LL5))

LL6 = 0
for (i in 1:Test_length) {
  row_Test_X2 = Test_X2[i, ]
  Test_X2b = row_Test_X2 %*% coefs3
  term = Test_Y[i]*log(invlogit(Test_X2b)) - (1-Test_Y[i])*K(Test_X2b)
  LL6 = LL6 + term
}
LL6 = LL6*-1/Test_length
print(paste("TennisLassoGLM1 Log Loss for Test Set:", LL6))

write(sprintf("Training Data Logistic Loss %.15f
Test Data Logistic Loss %.15f", LL5, LL6), file = "question 8 LogLoss.txt")
\end{lstlisting}
\subsubsection*{Code to Generate and Evaluate Model 4}
\begin{lstlisting}
column_names = rep("", 2*(length(tennis_names)-1))
X3 <- matrix(data=0, nrow=Training_length, ncol = length(column_names))
Test_X3 <- matrix(data=0, nrow=Test_length, ncol = length(column_names))
for (i in 2:length(tennis_names)) {
  column_names[2*i-3] <- tennis_names[i]
  column_names[2*i-2] <- paste(tennis_names[i], "Yrly.")
}
colnames(X3) <- column_names
colnames(Test_X3) <- column_names

Training_Date <- Training_Data$Date
Test_Date <- Test_Data$Date

for (i in 1:Training_length) {
  winner = Training_winners[i]
  loser = Training_losers[i]
  year = as.numeric(format(Training_Date[i],'%Y'))
  t = (year-2000)/(2014-2000)
  y_factor = 2*Training_Y[i]-1
  if (winner != tennis_names[1]) {
    X3[i, winner] <- 1*y_factor
    X3[i, paste(winner, "Yrly.")] <- t*y_factor
  }
  if (loser != tennis_names[1]) {
    X3[i, loser] <- -1*y_factor
    X3[i, paste(loser, "Yrly.")] <- -t*y_factor
  }
}

# Delete empty columns
X3 <- X3[, colSums(X3==0) != nrow(X3)]
# Make X3 into a sparse Matrix
X3 <- as(X3, "sparseMatrix")

TennisLassoGLM2 <- glmnet(X3, Training_Y, family="binomial", lambda = 0, 
                          intercept=FALSE, standardize = FALSE, thresh = 1e-9, maxit = 1e5)
temp_coefs = coef(TennisLassoGLM2, s = 0)
coefs4 <- as.vector(temp_coefs)
names(coefs4) <- rownames(temp_coefs)
coefs4 <- coefs4[-1]

write.table(coefs4, "question 9 coefs 1.txt", col.names = F, sep = ": ", quote = F)

for (i in 1:Test_length) {
  winner = Test_winners[i]
  loser = Test_losers[i]
  year = as.numeric(format(Test_Date[i],'%Y'))
  t = (year-2000)/(2014-2000)
  y_factor = 2*Test_Y[i]-1
  if (winner != tennis_names[1]) {
    Test_X3[i, winner] <- 1*y_factor
    Test_X3[i, paste(winner, "Yrly.")] <- t*y_factor
  }
  if (loser != tennis_names[1]) {
    Test_X3[i, loser] <- -1*y_factor
    Test_X3[i, paste(loser, "Yrly.")] <- -t*y_factor
  }
}
# Delete columns not in X3
Test_X3 <- Test_X3[, colnames(Test_X3) %in% colnames(X3)]

LL7 = 0
for (i in 1:Training_length) {
  row_X3 = X3[i, ]
  X3b = row_X3 %*% coefs4
  term = Training_Y[i]*log(invlogit(X3b)) - (1-Training_Y[i])*K(X3b)
  LL7 = LL7 + term
}
LL7 = LL7*-1/Training_length
print(paste("TennisLassoGLM2 Log Loss for Training Set:", LL7))

LL8 = 0
for (i in 1:Test_length) {
  row_X3 = X3[i, ]
  X3b = row_X3 %*% coefs4
  term = Test_Y[i]*log(invlogit(X3b)) - (1-Test_Y[i])*K(X3b)
  LL8 = LL8 + term
}
LL8 = LL8*-1/Test_length
print(paste("TennisLassoGLM2 Log Loss for Test Set:", LL8))

write(sprintf("Training Data Logistic Loss %.15f
Test Data Logistic Loss %.15f", LL7, LL8), file = "question 9 LogLoss noWeights.txt")
\end{lstlisting}
\subsubsection*{Code to Generate and Evaluate Model 5}
\begin{lstlisting}
w2 <- rep(c(0,1), length(tennis_names)-1)
names(w2) <- column_names
w2 <- w2[names(w2) %in% colnames(X3)]

TennisLassoGLM3 <- cv.glmnet(X3, Training_Y, family="binomial", alpha=1, penalty.factor = w2, 
                             intercept=FALSE, standardize = FALSE, thresh = 1e-9, maxit = 1e5)

temp_coefs = coef(TennisLassoGLM3, s = "lambda.min")
coefs5 <- as.vector(temp_coefs)
names(coefs5) <- rownames(temp_coefs)
coefs5 <- coefs5[-1]

write.table(coefs5, "question 9 coefs 2.txt", col.names = F, sep = ": ", quote = F)

LL9 = 0
for (i in 1:Training_length) {
  row_X3 = X3[i, ]
  X3b = row_X3 %*% coefs5
  term = Training_Y[i]*log(invlogit(X3b)) - (1-Training_Y[i])*K(X3b)
  LL9 = LL9 + term
}
LL9 = LL9*-1/Training_length
print(paste("TennisLassoGLM3 Log Loss for Training Set:", LL9))

LL10 = 0
for (i in 1:Test_length) {
  row_X3 = X3[i, ]
  X3b = row_X3 %*% coefs5
  term = Test_Y[i]*log(invlogit(X3b)) - (1-Test_Y[i])*K(X3b)
  LL10 = LL10 + term
}
LL10 = LL10*-1/Test_length
print(paste("TennisLassoGLM2 Log Loss for Test Set:", LL10))

write(sprintf("Training Data Logistic Loss %.15f
Test Data Logistic Loss %.15f", LL9, LL10), file = "question 9 LogLoss Weights.txt")
\end{lstlisting}
\subsubsection*{Code to Generate Figure \ref{fig:federer_vs_nadal}}
\begin{lstlisting}
Fed_Nad_Mat = matrix(0, nrow = 17, ncol=dim(X3)[2])
colnames(Fed_Nad_Mat) <- colnames(X3)
for (year in 2000:2016) {
  Fed_Nad_Mat[year - 1999, "Federer R."] = 1
  Fed_Nad_Mat[year - 1999, "Federer R. Yrly."] = (year-2000)/(2014-2000)
  Fed_Nad_Mat[year - 1999, "Nadal R."] = -1
  Fed_Nad_Mat[year - 1999, "Nadal R. Yrly."] = -(year-2000)/(2014-2000)
}
pdf("federer_nadal_probs.pdf")
plot(2000:2016, invlogit(Fed_Nad_Mat %*% coefs4), col = "blue", type="p", pch=19,
     xlab="Year", ylab="Probability Federer beats Nadal", ylim=c(0, 1))
lines(2000:2016, invlogit(Fed_Nad_Mat %*% coefs4), col = "blue", lty=1)
par(new=T)
plot(2000:2016, invlogit(Fed_Nad_Mat %*% coefs5), col = "red", type="p", pch=18,
     xlab="Year", ylab="Probability Federer beats Nadal", ylim=c(0, 1))
lines(2000:2016, invlogit(Fed_Nad_Mat %*% coefs5), col = "red", lty=2)
legend("topright", legend=c("No Lasso penalty", "Lasso Penalty"), col=c("blue", "red"),
       lty=1:2, pch = 19:18)
dev.off()
\end{lstlisting}
\subsubsection*{Code to Compare Models 1 to 5}
\begin{lstlisting}
KB1 = LL1 + length(coefs)/Training_length
KB2 = LL2 + length(coefs)/Test_length
KB3 = LL3 + length(coefs2)/Training_length
KB4 = LL4 + length(coefs2)/Test_length
KB5 = LL5 + length(coefs3)/Training_length
KB6 = LL6 + length(coefs3)/Test_length
KB7 = LL7 + length(coefs4)/Training_length
KB8 = LL8 + length(coefs4)/Test_length
KB9 = LL9 + length(coefs5)/Training_length
KB10 = LL10 + length(coefs5)/Test_length
print("The following are the log-loss and approximations of the Kullback Liebler Divergence of the models with the true distribution")
print("TennisGLM1")
print(paste("KB: Training Set:", KB1, "Test Set:", KB2))
print(paste("LL: Training Set:", LL1, "Test Set:", LL2))
print("TennisGLM2")
print(paste("KB: Training Set:", KB3, "Test Set:", KB4))
print(paste("LL: Training Set:", LL3, "Test Set:", LL4))
print("TennisLassoGLM1")
print(paste("KB: Training Set:", KB5, "Test Set:", KB6))
print(paste("LL: Training Set:", LL5, "Test Set:", LL6))
print("TennisLassoGLM2")
print(paste("KB: Training Set:", KB7, "Test Set:", KB8))
print(paste("LL: Training Set:", LL7, "Test Set:", LL8))
print("TennisLassoGLM3")
print(paste("KB: Training Set:", KB9, "Test Set:", KB10))
print(paste("LL: Training Set:", LL9, "Test Set:", LL10))

write.table(c("Model 1",
              paste("KB: Training Set:", KB1),
              paste("LL: Training Set:", LL1, "Test Set:", LL2),
              "Model 2",
              paste("KB: Training Set:", KB3),
              paste("LL: Training Set:", LL3, "Test Set:", LL4),
              "Model 3",
              paste("KB: Training Set:", KB5),
              paste("LL: Training Set:", LL5, "Test Set:", LL6),
              "Model 4",
              paste("KB: Training Set:", KB7),
              paste("LL: Training Set:", LL7, "Test Set:", LL8),
              "Model 5",
              paste("KB: Training Set:", KB9),
              paste("LL: Training Set:", LL9, "Test Set:", LL10)), file = "question 9 results.txt", sep = "\n", 
            row.names = F, col.names = F, quote = F)
\end{lstlisting}
\subsection*{Can you outperform the betting market?}
\subsubsection*{Code to generate the Markowitz Portfolio for Models 2 and 3}
\begin{lstlisting}
portfolio <- function(v, coeffs) {
  Bet_Data_2015 = Tennis[(Tennis$Date >= as.Date("2015/01/01")) &
                         (Tennis$Date <= as.Date("2015/12/31")) & 
                         (!is.na(Tennis$Date)) & 
                           (!is.na(Tennis$B365W)) & 
                           (!is.na(Tennis$B365L)), ]
  num_bets = dim(Bet_Data_2015)[1]
  q = rep(0, 2*num_bets)
  Q = matrix(0, 2*num_bets, 2*num_bets)
  for (i in 1:num_bets) {
    match = Bet_Data_2015[i, ]
    winner = match$Winner
    loser = match$Loser
    bet_win = as.numeric(match$B365W)
    bet_loss = as.numeric(levels(match$B365L))[match$B365L]
    surface = match$Surface
    
    match_vec = rep(0, dim(X2)[2])
    names(match_vec) <- colnames(X2)
    if (winner %in% names(match_vec)) {
      match_vec[winner] = 1
      if (paste(winner, surface) %in% names(match_vec)) {
        match_vec[paste(winner, surface)] = 1
      }
    }
    if (loser %in% names(match_vec)) {
      match_vec[loser] = -1
      if (paste(loser, surface) %in% names(match_vec)) {
        match_vec[paste(loser, surface)] = -1
      }
    }
    
    mu_i = as.numeric(invlogit(match_vec %*% coeffs))
    q[2*i-1] = (bet_win - 1)*mu_i
    q[2*i] = (bet_loss - 1)*(1 - mu_i)
    A_i = matrix(c((bet_win - 1)^2              , -(bet_win - 1)*(bet_loss - 1),
                   -(bet_win - 1)*(bet_loss - 1), (bet_loss - 1)^2             ),2,2, byrow = TRUE)
    Q[c(2*i-1, 2*i), c(2*i-1, 2*i)] = A_i*mu_i*(1-mu_i)
  }
  A_mat = rbind(matrix(rep(1, 2*num_bets), nrow=1), diag(2*num_bets))
  b_vec = c(1000, rep(0, 2*num_bets))
  # Note Q will always be singular so will find approximate solution using Q + 10^-8
  sol = solve.QP(2*v*(Q)+diag(x=1e-8, 2*num_bets), q, t(A_mat), b_vec, meq=1)
  # Note that Solve.QP isn't great as it may still have -ve entries in the solution
  # To fix this we will translate all values s.t they are >= 0, then rescale all values accordingly
  solu = sol$solution - min(sol$solution)
  solu = solu*1000/sum(solu)
  return(solu)
}
\end{lstlisting}
\subsubsection*{Code to Evaluate the Markowitz Portfolio generated by Models 2 and 3 and Generate Figure \ref{fig:markowitz_portfolio}}
\begin{lstlisting}
calc_portfolio_benefits <- function(pf) {
  Bet_Data_2015 = Tennis[(Tennis$Date >= as.Date("2015/01/01")) &
                           (Tennis$Date <= as.Date("2015/12/31")) & 
                           (!is.na(Tennis$Date)) & 
                           (!is.na(Tennis$B365W)) & 
                           (!is.na(Tennis$B365L)), ]
  num_bets = dim(Bet_Data_2015)[1]
  total = -1000
  for (i in 1:num_bets) {
    match = Bet_Data_2015[i, ]
    bet_win = as.numeric(match$B365W)
    total = total + bet_win*pf[2*i-1]
  }
  return(total)
}

v = 10^seq(-3, 3, length.out =7)
profits1 = rep(0,length(v))
profits2 = rep(0,length(v))
for (i in 1:length(v)) {
  profits1[i] <- calc_portfolio_benefits(portfolio(v[i], coefs2))
  profits2[i] <- calc_portfolio_benefits(portfolio(v[i], coefs3))
}

pdf("markowitz_portfolio.pdf")
plot(log10(v), profits1, type = "p", col="blue", pch=19,
     xlab=TeX("$\\log_{10}\\nu$"), ylab="Total Profit",
     ylim=c(-1000, 0))
lines(log10(v), profits1, col="blue", lty=1)
par(new=TRUE)
# points(v, profits2)
plot(log10(v), profits2, type = "p", col="red", pch=18,
     xlab=TeX("$\\log_{10}\\nu$"), ylab="Total Profit",
     ylim=c(-1000, 0))
lines(log10(v), profits2, col="red", lty=2)
legend("topright", legend = c("Model 2", "Model 3"), col=c("blue", "red"), lty=c(1, 2), pch=c(19, 18), inset=0.1)
dev.off()
\end{lstlisting}
\subsubsection*{Code to Generate Kelly Fractions using Models 2 and 3}
\begin{lstlisting}
kelly_fraction <- function(match, coeffs, rho, grid, debug) {
  winner = match$Winner
  loser = match$Loser
  bet_win = as.numeric(match$B365W)
  bet_loss = as.numeric(levels(match$B365L))[match$B365L]
  surface = match$Surface
  match_vec = rep(0, dim(X2)[2])
  names(match_vec) <- colnames(X2)
  if (winner %in% names(match_vec)) {
    match_vec[winner] = 1
    if (paste(winner, surface) %in% names(match_vec)) {
      match_vec[paste(winner, surface)] = 1
    }
  }
  if (loser %in% names(match_vec)) {
    match_vec[loser] = -1
    if (paste(loser, surface) %in% names(match_vec)) {
      match_vec[paste(loser, surface)] = -1
    }
  }
  mu_i = as.numeric(invlogit(match_vec %*% coeffs))
  opt_win_loss = c(0,0)
  opt_win_loss_val = -Inf
  for (Win in seq(0, 1, length.out=grid + 1)) {
    for (Loss in seq(0, 1, length.out=grid + 1)) {
      if (Win+Loss > 1) {
        break
      } else {
        val = log((1-Loss) + Win*(bet_win-1))*(mu_i) + log((1-Win) + Loss*(bet_loss-1))*(1-mu_i)
        if (!is.na(val) && val != -Inf && val > opt_win_loss_val) {
          opt_win_loss = c(Win, Loss)
          opt_win_loss_val = val
        }
      }
    }
  }
  return(rho*opt_win_loss)
}
\end{lstlisting}
\subsubsection*{Code to Evaluate the Kelly Fractions generated by Models 2 and 3 and Generate Figure \ref{fig:kelly_criterion}}
\begin{lstlisting}
eval_strat <- function(coeffs, rho, grid, bankroll){
  Bet_Data_2015 = Tennis[(Tennis$Date >= as.Date("2015/01/01")) &
                           (Tennis$Date <= as.Date("2015/12/31")) & 
                           (!is.na(Tennis$Date)) & 
                           (!is.na(Tennis$B365W)) & 
                           (!is.na(Tennis$B365L)), ]
  Bet_Data_2015 = Bet_Data_2015[order(Bet_Data_2015$Date), ]
  num_bets = dim(Bet_Data_2015)[1]
  b = rep(0, num_bets+1)
  b[1]=bankroll
  for (i in 1:num_bets) {
    match = Bet_Data_2015[i, ]
    fractions = kelly_fraction(match, coeffs, rho, grid, FALSE)
    bet_win = as.numeric(match$B365W)
    b[i+1] = b[i]*(1 - fractions[2] + fractions[1]*(bet_win-1))
  }
  return(c(b, as.Date("2015/1/1"), Bet_Data_2015$Date))
}
strat_results = eval_strat(coefs2, 0.1, 1000, 1000)
par(new = FALSE)
pdf("question 13 bankroll.pdf")
plot(as.Date(strat_results[-(1:length(strat_results)/2)], origin = "1970-01-01"), 
     strat_results[1:(length(strat_results)/2)], type="p",
     xlab = "Date", ylab = "Bankroll", pch=20, col="black")
lines(as.Date(strat_results[-(1:length(strat_results)/2)], origin = "1970-01-01"), 
      strat_results[1:(length(strat_results)/2)], lty=1, col="black")
dev.off()
\end{lstlisting}

\end{document}
