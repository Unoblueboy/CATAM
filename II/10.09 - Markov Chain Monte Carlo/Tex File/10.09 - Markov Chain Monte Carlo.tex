% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=2cm} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information
\usepackage[toc, page]{appendix}

\usepackage{float}
\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}
\lstset{ 
  language=R,                     % the language of the code
  basicstyle=\ttfamily, % the size of the fonts that are used for the code
  numbers=none,                   % removes the line-numbers
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=none,                     % no frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  keywordstyle=\color{RoyalBlue},      % keyword style
  commentstyle=\color{YellowGreen},   % comment style
  stringstyle=\color{ForestGreen}      % string literal style 
}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
% \usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{bm}
\usepackage{comment}


\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}


\begin{document}
\section*{10.9}
\vspace*{6cm}

\section*{10.9 Markov Chain Monte Carlo}

\subsection*{Gibbs Sampler}

\subsubsection*{Question 1}
We should note that by the assumption that
$$ \mathbf{X}\left(n\right) \xrightarrow{d} \mathbf{X} \sim \pi  $$
This implies that
$$ \lim_{n \to \infty}{\mathbb{P}\left(\mathbf{X}\left(n\right) = \mathbf{x}\right)} = \mathbb{P}\left(\mathbf{X} = \mathbf{x}\right) = \pi\left(\mathbf{x}\right) $$
Note that for any $\mathbf{y} \in S$,
 \begin{align*}
 	\mathbb{P}\left(\mathbf{X}\left(n+1\right) = \mathbf{y}\right) & = \sum_{x \in S}{\mathbb{P}\left(\mathbf{X}\left(n+1\right) = \mathbf{y} \mid \mathbf{X}\left(n\right) = \mathbf{x}\right)\mathbb{P}\left(\mathbf{X}\left(n\right) = \mathbf{x}\right)} \\
 	& = \sum_{x \in S}{\pi\left(\mathbf{x}, \mathbf{y}\right)\mathbb{P}\left(\mathbf{X}\left(n\right) = \mathbf{x}\right)}.
 \end{align*}
Here we use the law of total probability and that $\pi\left(\mathbf{x}, \mathbf{y}\right) = \mathbb{P}\left(\mathbf{X}\left(n+1\right) = \mathbf{y} \mid\mathbf{X}\left(n\right) = \mathbf{x}\right)$. We can now take limits of both sides and as $\left|S\right|<\infty$ we can take the limit inside the sum without problem, therefore
 $$ \pi\left(\mathbf{y}\right) = \sum_{x \in S}{\pi\left(\mathbf{x}, \mathbf{y}\right)\pi\left(\mathbf{x}\right)} .$$
So $\pi$ is an equilibrium distribution for this chain.

\subsection*{Football Data}

\subsubsection*{Question 2}

We are given the following prior distributions (and probability density functions)
\begin{alignat*}{5}
	& Y_{k,t} &\vert& \mu_k, \sigma_{k}^2 &\  \sim \ &  \mbox{N}\left(\mu_{k}, \sigma_{k}^{2}\right) &\qquad& f_{Y_{k,t} \vert \mu_k, \sigma_{k}^2}\left(y_{k, t}\right)&\ =\ &\left(2\pi\right)^{-\frac{1}{2}}\left(\sigma_k^{-2}\right)^{\frac{1}{2}}e^{-\frac{1}{2\sigma_k^{2}}\left(y_{k, t} - \mu_k\right)^2}\\
	& \mu_k&\vert& \theta &\  \sim \ & \mbox{N}\left(\theta, \sigma_{0}^{2}\right) &\qquad& f_{\mu_k\vert \theta}\left(\mu_{k}\right)&\ =\ &\frac{1}{\sqrt{2\pi\sigma_0^2}}e^{-\frac{1}{2\sigma_0^{2}}\left(\mu_{k} - \theta\right)^2}\\
	& \sigma_{k}^{-2} && &\  \sim \ & \Gamma\left(\alpha_0, \beta_0\right) &\qquad& f_{\sigma_{k}^{-2}}\left(\sigma_{k}^{-2}\right) &\ =\ & \frac{\beta_0^{\alpha_0}{\left(\sigma_{k}^{-2}\right)}^{\alpha_0 - 1}e^{-\beta_0{\sigma_{k}^{-2}}}}{\Gamma\left(\alpha_0\right)}\\
	& \theta && &\  \sim \ & \mbox{N}\left(\mu_0, \tau_{0}^{2}\right) &\qquad& f_{\theta}\left(\theta\right) &\ =\ &\frac{1}{\sqrt{2\pi\tau_0^2}}e^{-\frac{1}{2\tau_0^{2}}\left(\theta - \mu_{0}\right)^2}
.\end{alignat*}
True for $k = 1, \ldots, K,\  t=1, \ldots, T$, where $\sigma_0^2,\ \alpha_0,\ \beta_0,\ \mu_0,\ \tau_0^2$ are known constants. Now define the vectors $\boldsymbol{\sigma}^{-2}$, $\mathbf{Y}$, $\boldsymbol{\mu}$ such that $\boldsymbol{\sigma}^{-2}_{k} = \frac{1}{\sigma_{k}^{2}}$, $\mathbf{Y}_{k, t} = Y_{k,t}$, $\boldsymbol{\mu}_k = \mu_k$ for $k = 1, \ldots, K,\  t=1, \ldots, T$. Note that 
$$ \mathbf{Y}\,\vert\, \boldsymbol{\mu} , \boldsymbol{\sigma}^2 \overset{d}{=} \mathbf{Y}\,\vert\,\boldsymbol{\mu} , \boldsymbol{\sigma}^{-2} $$
We can thus write the pdf for the joint distribution of $\mathbf{Y}$, $\boldsymbol{\sigma}^{-2}$, $\boldsymbol{\mu}$, $\theta$ as follows

\begin{alignat*}{2}
	& f_{\mathbf{Y},\,\boldsymbol{\sigma}^{-2},\,\boldsymbol{\mu},\,\theta}\left(\mathbf{y},\,\boldsymbol{\sigma}^{-2},\,\boldsymbol{\mu},\,\theta\right) &\ =\ &
	\left(\prod_{k=1}^{K}{\left[\left(\prod_{t=1}^{T}{ f_{Y_{k,t} \vert \mu_k, \sigma_{k}^2}\left(y_{k, t}\right)}\right) f_{\mu_k\vert \theta}\left(\mu_{k}\right)  f_{\sigma_{k}^{-2}}\left(\sigma_{k}^{-2}\right) \right]}\right) f_{\theta}\left(\theta\right)\\
	& &\ =\ &  \left(\prod_{k=1}^{K}{\left[\left(2\pi\right)^{-\frac{T}{2}}\left(\sigma_k^{-2}\right)^{\frac{T}{2}}\exp\left\{ -\frac{1}{2}\left(\sigma_k^{-2}\right) \sum_{t=1}^{T}{\left(y_{k, t} - \mu_k\right)^2} \right\}\right.}\right. \\
	&&&  \qquad\left.{\left.\frac{1}{\sqrt{2\pi\sigma_0^2}}e^{-\frac{1}{2\sigma_0^{2}}\left(\mu_{k} - \theta\right)^2}  \frac{\beta_0^{\alpha_0}{\left(\sigma_{k}^{-2}\right)}^{\alpha_0 - 1}e^{-\beta_0{\sigma_{k}^{-2}}}}{\Gamma\left(\alpha_0\right)}\right]}\right)   \frac{1}{\sqrt{2\pi\tau_0^2}}e^{-\frac{1}{2\tau_0^{2}}\left(\theta - \mu_{0}\right)^2} \\
	& &\ \propto\ &  \left(\prod_{k=1}^{K}{\left(\sigma_k^{-2}\right)^{\frac{T}{2} + \alpha_0 -1}\exp\left\{-\frac{1}{2}\left(\sigma_k^{-2}\right) \sum_{t=1}^{T}{\left(y_{k, t} - \mu_k\right)^2} \right.}\right.\\
	&&& \quad\left.{\left.-\frac{1}{2\sigma_0^{2}}\left(\mu_{k} - \theta\right)^2 - \beta_0\left(\sigma_k^{-2}\right) - 1\right\}}\right)e^{-\frac{1}{2\tau_0^{2}}\left(\theta - \mu_{0}\right)^2}
\end{alignat*}

From this we can read off all the terms that contribute to all possible posterior distributions, in the one dimensional case

 \begin{alignat*}{2}
 	& \pi\left(\mu_k \mid \boldsymbol{\mu}_{-k},\, \theta, \boldsymbol{\sigma}^{2}, \mathbf{y}\right) &\ \propto\ & \exp\left\{-\frac{1}{2}\left[\left(T\sigma_k^{-2}+\sigma_0^{-2}\right)\mu_k^2 - 2\left(\sigma_k^{-2}\sum_{t=1}^{T}{y_{k, t}}+\sigma_0^{-2}\theta\right) \right]\right\} \\
 	& &\ \propto\ & \exp\left\{-\frac{1}{2\frac{1}{T\sigma_k^{-2}+\sigma_0^{-2}}}\left(\mu_k-\frac{\sigma_k^{-2}\sum_{t=1}^{T}{y_{k, t}}+\theta\sigma_0^{-2}}{T\sigma_k^{-2}+\sigma_0^{-2}}\right)^2\right\} \\
 	& \pi\left(\theta \mid \boldsymbol{\mu}, \boldsymbol{\sigma}^{2}, \mathbf{y}\right) &\ \propto\ & \exp\left\{-\frac{1}{2}\left[\left(K\sigma_0^{-2}+\tau_0^{-2}\right)\theta^2 - 2\left(\sigma_0^{-2}\sum_{k=1}^{K}{\mu_k}+\mu_0\tau_0^{-2}\right)\theta \right]\right\} \\
 	& &\ \propto\ & \exp\left\{-\frac{1}{2\frac{1}{K\sigma_0^{-2}+\tau_0^{-2}}}\left(\theta-\frac{\sigma_0^{-2}\sum_{k=1}^{K}{\mu_k}+\mu_0\tau_0^{-2}}{K\sigma_0^{-2}+\tau_0^{-2}}\right)^2\right\} \\
 	& \pi\left(\sigma_k^{-2} \mid \boldsymbol{\sigma}_{-k}^2, \boldsymbol{\mu}, \theta, \mathbf{y}\right) &\ \propto\ & \left(\sigma_k^{-2}\right)^{\frac{T}{2} + \alpha_0 -1}\exp\left\{-\left(\beta_0 + \frac{1}{2}\sum_{t=1}^{T}{\left(y_{k, t} - \mu_k\right)^2}\right)\sigma_k^{-2}\right\}
\end{alignat*}

Therefore

\begin{alignat*}{3}
	& \mu_k &\mid& \boldsymbol{\mu}_{-k},\, \theta, \boldsymbol{\sigma}^{2}, \mathbf{y} &\ \sim\ & \mbox{N}\left(\frac{\sigma_k^{-2}\sum_{t=1}^{T}{y_{k, t}}+\theta\sigma_0^{-2}}{T\sigma_k^{-2}+\sigma_0^{-2}}, \frac{1}{T\sigma_k^{-2}+\sigma_0^{-2}}\right) \\
	& \theta &\mid& \boldsymbol{\mu}, \boldsymbol{\sigma}^{2}, \mathbf{y} &\ \sim\ & \mbox{N}\left(\frac{\sigma_0^{-2}\sum_{k=1}^{K}{\mu_k}+\mu_0\tau_0^{-2}}{K\sigma_0^{-2}+\tau_0^{-2}}, \frac{1}{K\sigma_0^{-2}+\tau_0^{-2}}\right) \\
	& \sigma_k^{-2} &\mid& \boldsymbol{\sigma}_{-k}^2, \boldsymbol{\mu}, \theta, \mathbf{y} &\ \sim\ & \Gamma\left(\alpha_0 + \frac{T}{2}, \beta_0 + \frac{1}{2}\sum_{t=1}^{T}{\left(y_{k, t} - \mu_k\right)^2}\right)
\end{alignat*}.

To find the marginal prior distribution of  $\mu_k$, consider the density function for the joint prior distribution of $\mu_k$ and $\theta$

\begin{alignat*}{2}
& f_{\mu_k, \theta}\left(\mu_k, \theta\right) &\ =\ & f_{\mu_k\vert \theta}\left(\mu_{k}\right)f_{\theta}\left(\theta\right) \\
& &\ =\ &\frac{1}{\sqrt{2\pi\sigma_0^2}}e^{-\frac{1}{2\sigma_0^{2}}\left(\mu_{k} - \theta\right)^2}\frac{1}{\sqrt{2\pi\tau_0^2}}e^{-\frac{1}{2\tau_0^{2}}\left(\theta - \mu_{0}\right)^2} \\
& &\ =\ &\frac{1}{2\pi\sqrt{\sigma_0^2\tau_0^2}}\exp\left\{ -\frac{1}{2}\left[\left(\sigma_0^{-2}+\tau_0^{-2}\right)\theta^2  -2\left(\mu_k\sigma_0^{-2}+\mu_0\tau_0^{-2}\right)\theta +\left(\mu_k^2\sigma_0^{-2}+\mu_0^2\tau_0^{-2}\right) \right] \right\} \\
& &\ =\ & \frac{\sqrt{2\pi\left(\sigma_0^{-2}+\tau_0^{-2}\right)^{-1}}}{2\pi\sqrt{\sigma_0^2\tau_0^2}}\exp\left\{-\frac{1}{2}\left[ \left(\mu_k^2\sigma_0^{-2}+\mu_0^2\tau_0^{-2}\right) - \frac{\left( \mu_k\sigma_0^{-2}+\mu_0\tau_0^{-2} \right)^2}{\sigma_0^{-2}+\tau_0^{-2}} \right]\right\} \\
&&& \qquad\frac{1}{\sqrt{2\pi\left(\sigma_0^{-2}+\tau_0^{-2}\right)^{-1}}}\exp\left\{-\frac{1}{2\left(\sigma_0^{-2}+\tau_0^{-2}\right)^{-1}}\left(\theta - \frac{\mu_k\sigma_0^{-2}+\mu_0\tau_0^{-2}}{\sigma_0^{-2}+\tau_0^{-2}}\right)^2\right\} \\
& &\ =\ & \frac{1}{\sqrt{2\pi\left(\sigma_0^{2}+\tau_0^{2}\right)}}\exp\left\{ -\frac{1}{2\left(\sigma_0^{2}+\tau_0^{2}\right)}\left(\mu_k - \mu_0\right)^2 \right\}f_{\theta\mid\mu_k}\left(\theta\right) \\
& &\ =\ & f_{\mu_k}\left(\mu_k\right)f_{\theta\mid\mu_k}\left(\theta\right)
\end{alignat*}

where
\begin{alignat*}{2}
	& f_{\theta\mid\mu_k}\left(\theta\right) &\ =\ & \frac{1}{\sqrt{2\pi\left(\sigma_0^{-2}+\tau_0^{-2}\right)^{-1}}}\exp\left\{-\frac{1}{2\left(\sigma_0^{-2}+\tau_0^{-2}\right)^{-1}}\left(\theta - \frac{\mu_k\sigma_0^{-2}+\mu_0\tau_0^{-2}}{\sigma_0^{-2}+\tau_0^{-2}}\right)^2\right\}\\
	& f_{\mu_k}\left(\mu_k\right) &\ =\ & \frac{1}{\sqrt{2\pi\left(\sigma_0^{2}+\tau_0^{2}\right)}}\exp\left\{ -\frac{1}{2\left(\sigma_0^{2}+\tau_0^{2}\right)}\left(\mu_k - \mu_0\right)^2 \right\}.
\end{alignat*}
So we have now found the probability density functions for the prior distributions of $\theta\mid\mu_k$ and $\mu_k$, and can see that both are of the form of a normal distribution and thus deduce that $\mu_k$ has prior marginal distribution
\begin{equation*}
	\mu_k \sim \mbox{N}\left(\mu_0, \sigma_0^2 + \tau_0^2\right).
\end{equation*}

\subsubsection*{Question 3}
The code for the Gibbs Sampler can be found in the programs section

The following can be said for the assumptions made
\begin{itemize}
	\item $\sigma_0 = 10$
	\subitem $\sigma_0$ can be seen as how mean score varies from team to team, this assumption implies that average score of approximately $99.7\%$ of teams is within 30 points of the mean, this seams reasonable as when we look at the data, we see most of the data lies in the range $30$ to $90$.
	\item $\alpha_0 = 10^{-5}$, $\beta_0 = 10^{-3}$
	\subitem Allowing both $\alpha_0$ and $\beta_0$ to be small suggests that we know very little about the distribution of $\sigma_k$ prior to the process and thus allow mainly the data provided to determine the posterior distribution $\sigma_k$.
	\item $\mu_0=60$
	\subitem $\mu_0$ can be seen as the overall average score of all teams combined, so we have assumed that the average over all teams is 60
	\item $\tau_0=20$
	\subitem $\tau_0$ can be seen as the variance of the average score, so the above assumes that the average score over all teams could vary a lot, i.e we do not know much about their performance (as $\approx 99.7\%$ chance of lying within 3 standard deviations of the mean for a normal distribution, this along with the assumption $\mu_0=60$ says the mean score likely to be anywhere from 0 to 120, and as the minimum score is 0 and the maximum score is 114, this is a safe assumption)
\end{itemize}
We can initialise the Gibbs Sampler with naive approximations for all the variables i.e
\begin{alignat*}{2}
	& \mu_k &\ \approx\ & \frac{1}{T}\sum_{t=1}^{T}{Y_{k, t}} \\
	& \sigma_k^2 &\ \approx\ & \frac{1}{T}\sum_{t=1}^{T}{Y_{k, t}^2} - \left(\frac{1}{T}\sum_{t=1}^{T}{Y_{k, t}}\right)^2 \\
	& \theta &\ \approx\ & \frac{1}{KT}\sum_{t=1}^{T}{\sum_{k=1}^{K}{Y_{k, t}}}
\end{alignat*}
This choice of starting values should not matter in the long run as we expect the sampler to tend in distribution to the invariant distribution of all the parameters.

\subsubsection*{Question 4}
For our approximation of $\mathbb{E}\left[f\left(\mathbf{X}\right)\right]$ we make use of the fact
\begin{equation*}
	\sum_{n=1}^{N}{f\left(\mathbf{x}^n\right)} \xrightarrow{N \to \infty} \mathbb{E}\left[f\left(\mathbf{X}\right)\right]
\end{equation*}
so
\begin{equation}
	\mathbb{E}\left[f\left(\mathbf{X}\right)\right] \approx \sum_{n=1}^{N}{f\left(\mathbf{x}^n\right)} \label{eq:1}
\end{equation}
for $N$ sufficiently large, where $\mathbf{x}^n$ is the $n^{\text{th}}$ iteration of the Gibbs sampler, and $N$ is the total number of iterations. Let $\mathbf{X} = \boldsymbol{\mu}, \boldsymbol{\sigma}, \theta \mid \mathbf{y}$, then $\mathbf{x}^n$ represents the $n^{\text{th}}$ iteration of the Gibbs sampler programmed above, we will therefore denote 
$$\mathbf{x}^n = \left(\hat{\mu}_{1, n},\ldots , \hat{\mu}_{K, n}, \hat{\sigma^2}_{1, n},\ldots , \hat{\sigma^2}_{K, n}, \hat{\theta}_n\right) = \left(\hat{\boldsymbol{\mu}}_n, \hat{\boldsymbol{\sigma}^2}_n, \hat{\theta}_n\right),$$
then by taking $f\left(\mathbf{x}\right) = \mathbf{x}$ and looking at each entry, we get
\begin{alignat*}{2}
	& \mathbb{E}\left[\mu_k\mid\mathbf{y}\right] &\ \approx\ & \sum_{n=1}^{N}{\hat{\mu}_{k, n}} \\
	& \mathbb{E}\left[\sigma^2_k\mid\mathbf{y}\right] &\ \approx\ & \sum_{n=1}^{N}{\hat{\sigma^2}_{k, n}} \\
	& \mathbb{E}\left[\theta\mid\mathbf{y}\right] &\ \approx\ & \sum_{n=1}^{N}{\hat{\theta}_n} 
\end{alignat*}
i.e. approximations of the posterior mean for each parameter. Now if we consider $f\left(\mathbf{x}\right) = \mathbbm{1}_{\left\{a \leq \theta \leq b \right\}}$, then
$$\mathbb{P}\left(a \leq \theta \leq b \mid \mathbf{y} \right) \approx \sum_{n=1}^{N}{\mathbbm{1}_{\left\{a \leq \hat{\theta}_n \leq b \right\}}}$$
So we can choose a partition $a_0 < a_1 < \ldots < a_m$ such that $a_0 < \min{\hat{\theta}_n}$, $a_m > \max{\hat{\theta}_n}$ which is equispaced, we can then create a histogram using the above approximation and note that the probability that $\theta$ lies on the boundary of two boxes is $0$ so we can use the above approximation for $\mathbb{P}\left(a < \theta \leq b \mid \mathbf{y} \right)$ and $\mathbb{P}\left(a \leq \theta < b \mid \mathbf{y} \right)$. The R function \verb|hist| automatically does this with each partitions representing $ \mathbb{P}\left(a_i < \theta \leq a_{i+1} \mid \mathbf{y} \right) $ for $i > 0$ and $ \mathbb{P}\left(a_0 \leq \theta \leq a_1 \mid \mathbf{y} \right) $, when given the values of $\theta \mid \mathbf{y}$ and set \verb|freq = false|.\\

All of the posterior means were stored in a file, the output of which is shown below
\verbatiminput{"../R File/means.txt"}

And the graph for the distribution of $\theta \mid \mathbf{y}$ is saved in a pdf, the result is shown in Figure \ref{fig:post_theta_dist}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{"../R File/post_theta_dist"}
	\caption{Histogram for the approximation of distribution of  $\theta \mid \mathbf{y}$}
	\label{fig:post_theta_dist}
\end{figure}

\subsubsection*{Question 5}
To approximate $\mathbb{P}\left(\mu_k > \theta \mid \mathbf{y}\right)$ we can again use Equation \ref{eq:1} and the fact $\mathbb{P}\left(\mu_k > \theta \mid \mathbf{y}\right) = \mathbb{E}\left[\mathbbm{1}_{\left\{\mu_k > \theta\right\}} \mid \mathbf{y}\right]$ so
$$ \mathbb{P}\left(\mu_k > \theta \mid \mathbf{y}\right) \approx  \sum_{n=1}^{N}{\mathbbm{1}_{\left\{\hat{\mu}_{k, n}>\hat{\theta}_n\right\}}}$$
I chose to do this for all teams and store the results in a text file, the output of which is shown below
\verbatiminput{"../R File/probs.txt"}

\subsubsection*{Question 6}

I wish to look at the overall performance of this algorithm for all teams so instead of considering the sample variance for each team individually after $n$ iterations, I will instead consider the maximum sample variance of all the teams after $n$ iterations. Doing this I get the graphs shown in Figure \ref{fig:sample_vars_1}%
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/var_means"}
		\subcaption{Graph to show the maximum sample variance of Gibbs sampler approximation of the mean over all teams taken at regular intervals}
	\end{subfigure}%
	\hspace{0.04\textwidth}%
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/var_probs"}
		\subcaption{Graph to show the sample variance of Gibbs sampler approximation of the probability that a team is greater than average taken at regular intervals}
	\end{subfigure}	
	\caption{Graphs showing the performance of Gibbs sampler to approximate the mean and probability a team is greater than average over all teams}
	\label{fig:sample_vars_1}
\end{figure} and we see that the algorithm converges very quickly. To investigate the rate of convergence we will denote the maximum sample variance of the Gibbs sampler approximation of the mean after $n$ iterations as $s^2_{\mu, n}$ and the maximum sample variance of the Gibbs sampler approximation of the probability a team is greater than average after $n$ iterations $s^2_{\mathbb{P}, n}$. We will plot $\log s^2_{\mu, n}$ against $n$ and $\log s^2_{\mathbb{P}, n}$ against $n$ and look at the gradient, doing this we get the results shown in Figure \ref{fig:log_sample_vars}%
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/log_var_means"}
		\subcaption{Graph to show the log of the maximum sample variance of Gibbs sampler approximation of the mean over all teams taken at regular intervals}
	\end{subfigure}%
	\hspace{0.04\textwidth}%
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/log_var_probs"}
		\subcaption{Graph to show the log of the sample variance of Gibbs sampler approximation of the probability that a team is greater than average taken at regular intervals}
	\end{subfigure}	
	\caption{Graphs showing the rate of convergence of Gibbs sampler to approximate the mean and probability a team is greater than average over all teams}
	\label{fig:log_sample_vars}
\end{figure} and this suggests that the gradient is either
\begin{enumerate}
	\item converging to a constant value, $c < 0$
	\subitem This suggests that $\frac{s^2_{\mu, n}}{n^c} \xrightarrow{N \to \infty} d$ where $d$ is some non-zero constant, so $s^2_{\mu, n} \sim d n^c $
	\item converging to $0$ from below
	\subitem This would suggest that at a certain point doing more iterations won't lead to much of an improvement to the performance of our estimation.
\end{enumerate}

\subsubsection*{Question 7}

We have seen in the previous question that the algorithm seems to converge quickly to the values of $\mathbb{E}\left[\mu_k\mid \mathbf{y}\right]$ and $\mathbb{P}\left(\mu_k < \theta \mid \mathbf{y}\right)$, which suggests that the algorithm may converge quickly to the invariant distribution of the Markov chain, so we expect that

\begin{equation*}
	\left((\mathbf{X}\left(m_0\right), \ldots, \mathbf{X}\left(m_0+\left(n-1\right)\right)\right) \overset{d}{\approx} \left(\mathbf{X}\left(m_1\right), \ldots, \mathbf{X}\left(m_1+\left(n-1\right)\right)\right)
\end{equation*}

for $m_0, m_1, n \in \mathbb{N}$ where $\overset{d}{\approx}$ signifies approximate equality in distribution. So sample variances will be approximately independent of the number of pre-iterations we perform, which is what we see in Figure \ref{fig:sample_vars_3}.
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/var_means_3"}
		\subcaption{Graph to show the maximum sample variance of Gibbs sampler approximation of the mean over all teams taken at regular intervals for $n=2, 7, 27, 100$ iterations}
	\end{subfigure}%
	\hspace{0.04\textwidth}%
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/var_probs_3"}
		\subcaption{Graph to show the sample variance of Gibbs sampler approximation of the probability that a team is greater than average taken at regular intervals for $n=2, 7, 27, 100$ iterations}
	\end{subfigure}	
	\caption{Graphs showing the performance of Gibbs sampler to approximate the mean and probability a team is greater than average over all teams for $n=2, 7, 27, 100$ iterations}
	\label{fig:sample_vars_3}
\end{figure} This shows the number of pre-iterations we perform has little to no impact on the accuracy of our estimations, but that including more samples in our estimation improves the accuracy (which we saw already in question 6).

\subsubsection*{Question 8}

For this we will consider 200 different starting points with distributions decided by taking $\mu_k$, and $\theta$ from uniform distributions with minimum 0 and maximum 120, and taking $\sigma_k$ from uniform distributions with minimum 0.001 and maximum 40, and allow 200 iterations of the Gibbs Sampler.\\

We will first examine our estimates for the posterior distribution of $\theta$ using our new starting points compared to our original estimate. We can see how the new estimates performed in Figure \ref{fig:post_theta_dist_conv},
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{"../R File/theta_dist_conv"}	
	\caption{Graph showing the performance of Gibbs sampler to estimate the posterior distribution of $\theta$ for 5 different starting values, compared to the distribution shown in Figure \ref{fig:post_theta_dist}}
	\label{fig:post_theta_dist_conv}
\end{figure} where we have only looked at the results using the first 5 starting points to reduce clutter. We can see that the distributions have similar shape, but still vary between starting points. If we look at the values of $\theta$ in the first few iterations
\verbatiminput{"../R File/post_theta_iters_1.txt"}
and the last few iterations
\verbatiminput{"../R File/post_theta_iters_2.txt"}
We see that after the first one or two iterations the values seems to have quickly converged to some equilibrium distribution, suggesting that performing a small number of pre-iterations may increase performance, but doing much more will not result in much improvement.\\

We will now examine our estimates for the posterior means of $\mu_k$, $\sigma_k$, and our estimates of $\mathbb{P}\left(\mu_k < \theta \mid \mathbf{y}\right)$. We can do this by simply looking at the standard deviation of our estimates and look at the max standard deviation over all estimates. We need to be careful in observing that $\mu_k$, $\sigma_k$ and $\mathbb{P}\left(\mu_k < \theta \mid \mathbf{y}\right)$ are all of different magnitudes so the max standard deviation should be taken over all of them separately, doing this we get
\verbatiminput{"../R File/max_estimate_sds.txt"}
This suggests that our estimation is good in all the scenarios in which we used it except possibly for estimating $\sigma_k^2$, for which we can look at variance alongside the expected value.
\verbatiminput{"../R File/sigma_sds.txt"}
We see that the compared to the expectation, the standard deviation is relatively small, suggesting our estimates for $\sigma_k^2$ are good.\\

Now we can see that in the case where we randomly guess a starting point, the use of pre-iterations in some cases leads to a noticeable improvement in our estimation, but in other cases we do not see much of improvement, seen in Figure \ref{fig:max_var_estimates}. This suggests that although pre-sampling may improve the accuracy of the estimation, it would be much more beneficial to simply use more samples in our estimation of the parameters.
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/max_var_estimates_1"}
		\subcaption{Graph to show the maximum sample variance of Gibbs sampler approximation of the posterior mean of $\mu_k$ over all teams using $m$ pre-iterations and 100 sample iterations}
	\end{subfigure}%
	\hspace{0.04\textwidth}%
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/max_var_estimates_2"}
		\subcaption{Graph to show the maximum sample variance of Gibbs sampler approximation of the posterior mean of $\sigma_k^2$ over all teams using $m$ pre-iterations and 100 sample iterations}
	\end{subfigure}	
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/max_var_estimates_3"}
		\subcaption{Graph to show the sample variance of Gibbs sampler approximation of the posterior mean of $\theta$ using $m$ pre-iterations and 100 sample iterations}
	\end{subfigure}%
	\hspace{0.04\textwidth}%
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{"../R File/max_var_estimates_4"}
		\subcaption{Graph to show the maximum sample variance of Gibbs sampler approximation of the probability that a team is greater than average using $m$ pre-iterations and 100 sample iterations}
	\end{subfigure}	
	\caption{Graphs showing the accuracy of Gibbs Sampler in the approximations it makes, and how it varies depending on the number of pre-iteration performed}
	\label{fig:max_var_estimates}
\end{figure}


\clearpage
\section*{Program Listings}
\subsection*{Code to set up the Gibbs Sampler, as well as the Gibbs Sampler}
\begin{lstlisting}
params <- rep(0, 5)
names(params) <- c("theta_0", "alpha_0", "beta_0", "mu_0", "tau_0")
params["sigma_0"] = 10
params["alpha_0"] = 1e-5
params["beta_0"] = 1e-3
params["mu_0"] = 60
params["tau_0"] = 20

gibbs_sampler <- function(data, start, iter, d_params) {
  # note that in the data the columns are different times
  # while the rows are different teams
  num_teams = dim(data)[1]
  num_years = dim(data)[2]
  
  sum_data_over_time = rowSums(data)
  
  result = matrix(c(start, rep(0,iter*(2*num_teams+1))),  
                  ncol=2*num_teams+1, byrow = TRUE)
  for (i in 1:iter) {
    cur_sol = result[i, ]
    # Calculate all mu's
    for (k1 in 1:num_teams) {
      sigma_k_squared = cur_sol[num_teams+k1]
      dist_var = 1/(num_years*sigma_k_squared^(-1)+d_params["sigma_0"]^(-2))
      dist_mu = (sum_data_over_time[k1]*sigma_k_squared^(-1) + 
                   d_params["sigma_0"]^(-2)*cur_sol[2*num_teams+1])*dist_var
      cur_sol[k1] = rnorm(1, mean = dist_mu, sd = sqrt(dist_var))
    }
    # Calculate all sigma
    faux_var = rowSums((data-cur_sol[1:num_teams])^2)
    for (k2 in 1:num_teams) {
      alpha_i = d_params["alpha_0"] + num_years/2
      beta_i = d_params["beta_0"] + (1/2)*faux_var[k2]
      cur_sol[num_teams + k2] = 1/rgamma(1, shape = alpha_i, rate = beta_i)
    }
    # calculate theta
    dist_var = 1/(num_teams*d_params["sigma_0"]^(-2)+d_params["tau_0"]^(-2))
    dist_mu = (d_params["sigma_0"]^(-2)*sum(cur_sol[1:num_teams])-
                 d_params["mu_0"]*d_params["tau_0"]^(-2))*dist_var
    cur_sol[2*num_teams + 1] = rnorm(1, mean = dist_mu, sd = sqrt(dist_var))
    result[i+1, ] = cur_sol
  }
  # Just for my own readability
  result_colnames = c(
    unname(sapply(rownames(data), function(x){paste(x, "mean")})),
    unname(sapply(rownames(data), function(x){paste(x, "variances")})),
    "theta"
  )
  colnames(result) <- result_colnames
  return(result)
}


# We made the starting point estimations of each of the parameters
# i.e starting means is the mean score of team 
starting_means = rowMeans(Football)
# starting vars is the vars of each team according to the data
starting_vars = rowMeans(Football^2)-starting_means^2
# starting theta is the mean of all scores
starting_theta = mean(starting_means) # valid as all rows have same number of entries
gibbs_start = c(starting_means, starting_vars, starting_theta)
gibbs_start = unname(gibbs_start)

iterations = 10000

result = gibbs_sampler(Football, gibbs_start, iterations, params)
K = dim(Football)[1]
\end{lstlisting}
\subsection*{Code to generate all outputs in Question 4}
\begin{lstlisting}
post_mean = colMeans(result)
mean_strings = paste("The posterior mean of",names(post_mean), ":", post_mean)
write.table(mean_strings, file = "means.txt", sep="\n", row.names = FALSE, col.names = FALSE, quote=FALSE)
post_theta_samples = result[, 2*K+1]
pdf("post_theta_dist.pdf")
post_theta_hist_info = hist(post_theta_samples, freq = F, xlab=TeX("$\\theta$"), main = TeX("Histogram of the posterior distribution of $\\theta$"), ylim=c(0,0.14))
dev.off()
\end{lstlisting}
\subsection*{Code to generate all outputs in Question 5}
\begin{lstlisting}
apply(result, 1, function(x) {as.numeric(x[1]>x[2*K+1])})
prob_mu_gt_theta = colMeans(t(apply(result, 1, function(x) {x[1:K]>x[2*K+1]})))
names(prob_mu_gt_theta) <- rownames(Football)
prob_strings = paste("Probability mean score of", names(prob_mu_gt_theta), 
                     "is greater than average is:", prob_mu_gt_theta)
write.table(prob_strings, file = "probs.txt", sep="\n", row.names = FALSE, col.names = FALSE, quote=FALSE)
\end{lstlisting}
\subsection*{Code to generate all outputs in Question 6}
\begin{lstlisting}
N = 3000
trials = 100
trial_results = array(0, dim = c(N+1, 2*K+1, trials))
for (i in 1:trials) {
  trial_results[,,i] = gibbs_sampler(Football, gibbs_start, N, params)
}

# Points represents the number of points I want to sample e.g. If I want to sample at
# 5 different values of N from N = 2 to 1001, then points = 5
points = 40
var_mat_probs = matrix(0, nrow = points, ncol = K)
var_mat_mus = matrix(0, nrow = points, ncol = K)
colnames(var_mat_probs) <- rownames(Football)
# n_seq is a list representing the stopping point of several trials that is to simulate 5 
# trials with N = 5, 10, 15, 20, can do 1 trial of length 20, then just look at the data points
# up until N = 5, 10, 15, 20
n_seq = floor(seq(2,N+1, length.out = points))

for (i in 1:points) {
  n = n_seq[i]
  # Here we are approximating P(mu_k > theta) with the given data (i.e iterations 1 to n)
  probs_mu_gt_theta = apply(trial_results[1:n,,], 3, function(x) {
  colMeans(t(apply(x, 1, function(z) {
    z[1:K]>z[2*K+1]
    })))
  })
  # Here we are approximating mu_k with the given data (i.e iterations 1 to n)
  mus = apply(trial_results[1:n,,], 3, function(x) {
    colMeans(x[, 1:K])
  })
  
  var_probs_mu_gt_theta = rowMeans(probs_mu_gt_theta^2)-rowMeans(probs_mu_gt_theta)^2
  var_mat_probs[i,] = var_probs_mu_gt_theta
  var_mus = rowMeans(mus^2)-rowMeans(mus)^2
  var_mat_mus[i,] = var_mus
}

# We could look at indvidual team k, but we could also look at all teams then consider the 
# maximum variance, thus looking at some kind of uniform convergence
pdf("var_probs.pdf")
max_var_probs = apply(var_mat_probs, 1, max)
plot(n_seq, max_var_probs, type="l",
     xlab="n", ylab="Maximum Variance over all teams (probabilities)")
dev.off()

pdf("var_means.pdf")
max_var_mus = apply(var_mat_mus, 1, max)
plot(n_seq, max_var_mus, type="l",
     xlab="n", ylab="Maximum Variance over all teams (means)")
dev.off()


# Now to investigate the relation between the variance and n
pdf("log_var_probs.pdf")
log_max_var_probs = log(max_var_probs)
plot(n_seq, log_max_var_probs, type="l",
     xlab="n", ylab="Log Maximum Variance over all teams (probabilities)")
dev.off()

pdf("log_var_means.pdf")
log_max_var_mus = log(max_var_mus)
plot(n_seq, log_max_var_mus, type="l",
     xlab="n", ylab="Log Maximum Variance over all teams (means)")
dev.off()
\end{lstlisting}
\subsection*{Code to generate all outputs in Question 7}
\begin{lstlisting}
min_M = 100
max_M = 3000
num_M = 5
M_seq = floor(seq(min_M, max_M, length.out = num_M))
N = 1000
trials = 100
trial_results = array(0, dim = c(N+max_M+1, 2*K+1, trials))
for (i in 1:trials) {
  trial_results[,,i] = gibbs_sampler(Football, gibbs_start, N+max_M, params)
}

points = 40
var_mat_probs = array(0, dim = c(points, num_M, K))
var_mat_mus = array(0, dim = c(points, num_M, K))
n_seq = floor(seq(2,N+1, length.out = points))
for (i in 1:points) {
  for (j in 1:num_M) {
    n = n_seq[i]
    m = M_seq[j]
    probs_mu_gt_theta = apply(trial_results[(m+1):(m+n),,], 3, function(x) {
      colMeans(t(apply(x, 1, function(z) {
        z[1:K]>z[2*K+1]
      })))
    })
    mus = apply(trial_results[(m+1):(m+n),,], 3, function(x) {
      colMeans(x[, 1:K])
    })
    var_probs_mu_gt_theta = rowMeans(probs_mu_gt_theta^2)-rowMeans(probs_mu_gt_theta)^2
    var_mat_probs[i, j, ] = var_probs_mu_gt_theta
    var_mus = rowMeans(mus^2)-rowMeans(mus)^2
    var_mat_mus[i, j, ] = var_mus
  }
}

# We could look at indvidual team k, but we could also look at all teams then consider the 
# maximum variance, thus looking at some kind of uniform convergence
cols = rainbow(num_M)
pdf("var_probs_2.pdf")
matplot(t(matrix(rep(n_seq,num_M), nrow=num_M, byrow = T)), 
        apply(var_mat_probs, 2, function(x) {apply(x, 1, max)}), type="l",
        col = cols, xlab="n", ylab="Maximum Variance over all teams (probabilities)")
legend("topright", legend = M_seq, col=cols, lwd=2, ncol=2, cex=0.8, title = "Number, M,  of pre-iterations")
dev.off()

pdf("var_means_2.pdf")
matplot(t(matrix(rep(n_seq,num_M), nrow=num_M, byrow = T)), 
        apply(var_mat_mus, 2, function(x) {apply(x, 1, max)}), type="l",
     xlab="n", ylab="Maximum Variance over all teams (means)")
legend("topright", legend = M_seq, col=cols, lwd=2, ncol=2, cex=0.8, title = "Number, M,  of pre-iterations")
dev.off()


min_M = 1
max_M = 1000
num_M = 100
M_seq = floor(seq(min_M, max_M, length.out = num_M))
N = 100
trials = 100
trial_results = array(0, dim = c(N+max_M+1, 2*K+1, trials))
for (i in 1:trials) {
  trial_results[,,i] = gibbs_sampler(Football, gibbs_start, N+max_M, params)
}

points = 4
var_mat_probs = array(0, dim = c(points, num_M, K))
var_mat_mus = array(0, dim = c(points, num_M, K))
# colnames(var_mat_probs) <- rownames(Football)
n_seq = round(exp(seq(log(2),log(N), length.out=points)))
for (i in 1:points) {
  for (j in 1:num_M) {
    n = n_seq[i]
    m = M_seq[j]
    probs_mu_gt_theta = apply(trial_results[(m+1):(m+n),,], 3, function(x) {
      colMeans(t(apply(x, 1, function(z) {
        z[1:K]>z[2*K+1]
      })))
    })
    mus = apply(trial_results[(m+1):(m+n),,], 3, function(x) {
      colMeans(x[, 1:K])
    })
    var_probs_mu_gt_theta = rowMeans(probs_mu_gt_theta^2)-rowMeans(probs_mu_gt_theta)^2
    var_mat_probs[i, j, ] = var_probs_mu_gt_theta
    var_mus = rowMeans(mus^2)-rowMeans(mus)^2
    var_mat_mus[i, j, ] = var_mus
  }
}

cols = rainbow(points)
pdf("var_probs_3.pdf")
matplot(M_seq, 
        apply(var_mat_probs, 1, function(x) {apply(x, 1, max)}), type=rep("p", points),
        col = cols, xlab="m", ylab="Maximum Variance over all teams (probabilities)", pch=rep(1, points))
legend("right", bg="white", legend = n_seq, col=cols, pch=rep(1, points), ncol=2, cex=0.8, title = "Number, n, of iterations")
dev.off()

pdf("var_means_3.pdf")
matplot(M_seq, 
        apply(var_mat_mus, 1, function(x) {apply(x, 1, max)}), type=rep("p", points),
        col = cols, xlab="m", ylab="Maximum Variance over all teams (means)", pch=rep(1, points))
legend("topright", bg="white", legend = n_seq, col=cols, pch=rep(1, points), ncol=2, cex=0.8, title = "Number, n, of iterations")
dev.off()
\end{lstlisting}
\subsection*{Code to generate all outputs in Question 6}
\begin{lstlisting}
print("Question 8")
num_start_points = 200
iterations=200
max_pre_iterations = 500
num_theta_analysis = 5

starting_points = matrix(
  c(runif(K*num_start_points, 0, 120),
    runif(K*num_start_points, 0.001,40),
    runif(num_start_points, 0, 120)), nrow = 2*K+1, byrow = TRUE)

all_data = array(0, dim=c(num_start_points, max_pre_iterations+ iterations+1, 2*K+1))

for (i in 1:num_start_points) {
  data = gibbs_sampler(Football, starting_points[, i], max_pre_iterations + iterations, params)
  all_data[i, , ]=data
}

write.table(all_data[1:5,1:5 ,2*K+1 ], file = "post_theta_iters_1.txt", row.names=FALSE, col.names=FALSE)
write.table(all_data[1:5,(iterations - 3):(1+iterations) ,2*K+1 ], file = "post_theta_iters_2.txt", row.names=FALSE, col.names=FALSE)

pdf("theta_dist_conv.pdf")
plot(post_theta_hist_info, freq=FALSE, ylim = c(0, 0.14), 
     xlab=TeX("$\\theta$"), main = TeX("Histogram of the posterior distribution of $\\theta$"))
cols = rainbow(num_theta_analysis)
for (i in 1:num_theta_analysis) {
  break_points = seq(floor(min(all_data[i,1:(1+iterations) , 2*K+1])) - (floor(min(all_data[i,1:(1+iterations) , 2*K+1]))%%2),
                     ceiling(max(all_data[i,1:(1+iterations) , 2*K+1])) + (ceiling(max(all_data[i,1:(1+iterations) , 2*K+1]))%%2),
                     by=2)
  hist_data = hist(all_data[i,1:(1+iterations), 2*K+1], plot = FALSE, breaks = break_points)
  lines(hist_data$mids, hist_data$density, col=cols[i])
}
dev.off()

means = t(apply(all_data[,1:(1+iterations),], 1, function(x) {colMeans(x)}))
probs = t(apply(all_data[,1:(1+iterations),], 1, function(x) {
  rowMeans(
    apply(x, 1, function(z) {
      z[1:K] > z[2*K + 1]
    })
  )
}))

all_estimates = cbind(means, probs)
all_variance = colMeans(all_estimates^2)-colMeans(all_estimates)^2
max_sds = c(max(all_variance[1:K]),
             max(all_variance[(K+1):(2*K)]),
             max(all_variance[2*K+1]),
             max(all_variance[(2*K+2):(3*K+1)]))^0.5

write.table(c(paste("Maximum standard deviation for posterior mean of mu_k is", max_sds[1]),
              paste("Maximum standard deviation for posterior mean of sigma_k^2 is", max_sds[2]),
              paste("Maximum standard deviation for posterior mean of theta is", max_sds[3]),
              paste("Maximum standard deviation for posterior probability mu_k > theta is", max_sds[4]))
          ,file = "max_estimate_sds.txt", sep="\n", row.names = FALSE, col.names = FALSE, quote=FALSE)

write.table(t(rbind(colMeans(all_estimates)[(K+1):(2*K)], all_variance[(K+1):(2*K)]^0.5)),
            file = "sigma_sds.txt", sep=" ", row.names = FALSE, col.names = c("Expected value", "Standard Deviation"), quote=FALSE)

pre_iter_max_vars = matrix(rep(c(0,0,0,0), max_pre_iterations+1), nrow=max_pre_iterations+1)

for (i in 0:max_pre_iterations) {
  means = t(apply(all_data[,(i+1):(i+1+iterations),], 1, function(x) {colMeans(x)}))
  probs = t(apply(all_data[,(i+1):(i+1+iterations),], 1, function(x) {
    rowMeans(
      apply(x, 1, function(z) {
        z[1:K] > z[2*K + 1]
      })
    )
  }))
  
  all_estimates = cbind(means, probs)
  all_variance = colMeans(all_estimates^2)-colMeans(all_estimates)^2
  max_vars = c(max(all_variance[1:K]),
              max(all_variance[(K+1):(2*K)]),
              max(all_variance[2*K+1]),
              max(all_variance[(2*K+2):(3*K+1)]))
  pre_iter_max_vars[i+1, ] = max_vars
}

pdf("max_var_estimates_1.pdf")
plot(0:max_pre_iterations, pre_iter_max_vars[, 1], ylim=c(0,0.7), pch=1, xlab="m", ylab="Maximum Variance over all teams (means)")
dev.off()
pdf("max_var_estimates_2.pdf")
plot(0:max_pre_iterations, pre_iter_max_vars[, 2], ylim=c(0,5000), pch=1, xlab="m", ylab="Maximum Variance over all teams (variance)")
dev.off()
pdf("max_var_estimates_3.pdf")
plot(0:max_pre_iterations, pre_iter_max_vars[, 3], ylim=c(0,0.1), pch=1, xlab="m", ylab="Variance over all teams (theta)")
dev.off()
pdf("max_var_estimates_4.pdf")
plot(0:max_pre_iterations, pre_iter_max_vars[, 4], ylim=c(0,0.002), pch=1, xlab="m", ylab="Maximum Variance over all teams (probs)")
dev.off()
\end{lstlisting}

\end{document}
